{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### <img align=\"left\" src=\"img/ECE364-logo.png\" width=\"300px\" style=\"padding:30px;border:thin solid white;\"> \n",
    "\n",
    "# Lecture 8 - Linear Classification II\n",
    "## ECE364 - Programming Methods for Machine Learning\n",
    "### Nickvash Kani \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### Slides based off prior lectures by Alex Schwing, Aigou Han, Farzad Kamalabadi, Corey Snyder. All mistakes are my own!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Things we will cover: \n",
    "\n",
    "- Multi-parameter linear regression\n",
    "- Polnomial regression \n",
    "- Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-parameter linear regression\n",
    "\n",
    "Last lecture we discussed linear regression and specifically we focused on one-dimensional linear fits: \n",
    "\n",
    "$$\n",
    "y = mx + b\n",
    "$$\n",
    "\n",
    "This is called **simple linear regression**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When we have multiple input parameters that we wish to regress on: \n",
    "\n",
    "$$\n",
    "y = m_nx_n \\ldots + m_3x_3 + m_2x_2 + m_1x_1 + b\n",
    "$$\n",
    "\n",
    "This is called **multiple linear regression**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dataset we are artificially creating: \n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 10 x values between 0 and 10\n",
    "DN=11\n",
    "x = torch.linspace(0, 10, DN)\n",
    "y = torch.linspace(0, 10, DN)\n",
    "points = torch.stack(torch.meshgrid(x, y)).T.reshape(-1,2)\n",
    "X = torch.column_stack((torch.ones(points.shape[0]), points))\n",
    "N = X.shape[0]\n",
    "#print(X)\n",
    "\n",
    "# Generate true target values with some noise\n",
    "true_b = 3\n",
    "true_m1 = 3\n",
    "true_m2 = 1\n",
    "\n",
    "t = true_b + true_m1*X[:,1] + true_m2*X[:,2] + 2.0*torch.randn(X.shape[0], dtype=X.dtype)\n",
    "\n",
    "# initialize guesses for w, b\n",
    "w_gd = torch.randn((3), requires_grad=True) # size (1,)\n",
    "print('Initial guesses: w0={:.6f}, w1={:.6f}, w2={:.6f}'.format(w_gd[0].data, w_gd[1].data, w_gd[2].data))\n",
    "\n",
    "# information for tracking\n",
    "b_vals = [w_gd[0].data.item()]\n",
    "w1_vals = [w_gd[1].data.item()]\n",
    "w2_vals = [w_gd[2].data.item()]\n",
    "\n",
    "# gradient descent loop\n",
    "n_iter = 10000 # number of iterations\n",
    "alpha = 10e-3 # step size\n",
    "for n in range(n_iter):\n",
    "    errors = t-(X@w_gd)\n",
    "    loss = torch.sum((errors)**2)/N\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w_gd -= alpha*w_gd.grad\n",
    "        w_gd.grad = None\n",
    "        \n",
    "    # log information\n",
    "    w2_vals.append(w_gd[2].data.item())\n",
    "    w1_vals.append(w_gd[1].data.item())\n",
    "    b_vals.append(w_gd[0].data.item())\n",
    "    \n",
    "# examine solution\n",
    "print('Final guesses: w0={:.6f}, w1={:.6f}, w2={:.6f}'.format(w_gd[0].data, w_gd[1].data, w_gd[2].data))\n",
    "\n",
    "iter_num = np.array([0, 50, 100, 250, 500]).astype(int)\n",
    "plt.figure(figsize=(20, 5))\n",
    "for j, i in enumerate(iter_num):\n",
    "    ax = plt.subplot(1, 5, j+1, projection='3d')\n",
    "    ax.scatter(X[:,1].detach().numpy(), X[:,2].detach().numpy(), t, color='green')\n",
    "    xx, yy = torch.meshgrid(x, y)\n",
    "    curr_fn = w2_vals[i]*yy + w1_vals[i]*xx + b_vals[i]\n",
    "    ax.plot_surface(xx, yy, curr_fn.detach().numpy(), color='blue', alpha=0.5)\n",
    "    plt.grid(True)\n",
    "    plt.title('Regressed function: Iteration {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "W = torch.linalg.inv(X.t()@X)@X.t()@t\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear regress all the things!\n",
    "\n",
    "<img align=\"center\" src=\"./img/Linear_regress_everything.png\" width=\"600px\" style=\"padding:30px;border:thin solid white;\"> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import torch\n",
    "\n",
    "# Load the dataset\n",
    "california_housing = fetch_california_housing()\n",
    "\n",
    "# Extract features and target\n",
    "X = california_housing.data\n",
    "t = california_housing.target\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "t_tensor = torch.tensor(t, dtype=torch.float32).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Feature names\n",
    "feature_names = california_housing.feature_names\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each feature against the target\n",
    "for i, feature in enumerate(feature_names):\n",
    "    axes[i].scatter(X[:, i], t, alpha=0.5)\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Median House Value')\n",
    "    axes[i].set_title(f'{feature} vs. Median House Value')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "N = X_tensor.shape[0]\n",
    "X = torch.column_stack((torch.ones(N), X_tensor))\n",
    "\n",
    "w_gd = torch.randn((X_tensor.shape[1]+1), requires_grad=True) # size (1,)\n",
    "print('Initial guesses: ' + str(w_gd))\n",
    "# information for tracking\n",
    "w_vals = []\n",
    "w_vals.append(w_gd.detach().numpy())\n",
    "loss_vals = []\n",
    "\n",
    "# gradient descent loop\n",
    "n_iter = 2 # number of iterations\n",
    "alpha = 0.01e-3 # step size\n",
    "for n in range(n_iter):\n",
    "    # compute loss function (objective function)\n",
    "    errors = t_tensor-(X@w_gd)\n",
    "    loss = torch.sum((errors)**2)/N\n",
    "    # backpropagate gradients\n",
    "    loss.backward()\n",
    "    # perform gradient descent update step\n",
    "    with torch.no_grad():\n",
    "        # don't want the gradient update step to accumulate further gradients at a, b, and c\n",
    "        w_gd -= alpha*w_gd.grad\n",
    "        # manually zero out the gradients before next backward pass\n",
    "        w_gd.grad = None\n",
    "        \n",
    "    # log information\n",
    "    loss_vals.append(loss.item()) # log MSE\n",
    "    w_vals.append(w_gd.detach().numpy())\n",
    "    \n",
    "# examine solution\n",
    "print('Final guesses: ' + str(w_gd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(feature_names)\n",
    "W = torch.linalg.inv(X.t()@X)@X.t()@t_tensor\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img align=\"center\" src=\"./img/Linear_regress_maybe_not.png\" width=\"600px\" style=\"padding:30px;border:thin solid white;\"> \n",
    "\n",
    "* Credit to Allie Brosh from [Hyperbole and a Half](https://hyperboleandahalf.blogspot.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Polynomial fitting\n",
    "\n",
    "Suppose we had the following plot: \n",
    "\n",
    "<img align=\"middle\" src=\"./img/scatter_plot_with_sample_polynomial_no_lines.png\" width=\"500px\" style=\"padding:30px;border:thin solid white;\"> \n",
    "\n",
    "Linear model here would probably be less than ideal...... what do we do? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Linear model here is less than ideal...... what do we do? \n",
    "\n",
    "<img align=\"middle\" src=\"./img/poly_fit_1.png\" width=\"500px\" style=\"padding:30px;border:thin solid white;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Well think of the last few lectures. We had plenty of complex functions we fitted our data to .... What were we doing then?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A complex polynomial is simply the combination of a bunch of powers of $x$. So if we can combine linear functions together, why not combine polynomial function together also?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We do this by augmenting our prior work using a feature map: \n",
    "\n",
    "$$\\phi(x) = \\begin{pmatrix} 1 \\\\ x \\\\ x^2 \\\\ x^3 \\end{pmatrix}$$\n",
    "\n",
    "making the regression model: \n",
    "\n",
    "$$ y = w^T \\phi(x) $$\n",
    "\n",
    "All of the derivations in the lecture so far are going to remain the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dataset we are artificially creating: \n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "N=10\n",
    "# Generate 10 x values between 0 and 10\n",
    "x = torch.linspace(-10, 10, N)\n",
    "\n",
    "# Generate y values with some variability\n",
    "def f(x): \n",
    "    return 0.25 * x **3 + -0.5 * x**2 + -10 * x + 20 \n",
    "\n",
    "t = torch.clone(f(x) + np.random.normal(0, 15, size=len(x)))\n",
    "\n",
    "#Set polynomial order (2 means we want a fitting function w3 x^3 + w2 x^2 + w1 x + w0)\n",
    "order = 3\n",
    "\n",
    "def f_fit(w, x):\n",
    "    out = w[0]\n",
    "    for i in (1+np.arange(order)):\n",
    "        a = w[i] * x**i\n",
    "        b = out\n",
    "        out = a + b \n",
    "    return out\n",
    "\n",
    "# initialize guesses for w, b\n",
    "w_gd = torch.randn((order+1), requires_grad=True) # size (1,)\n",
    "print('Initial guesses: ' + str(w_gd))\n",
    "\n",
    "n_iter = 200000 # number of iterations\n",
    "alpha = 1e-6 # step size\n",
    "momentum = 0.9\n",
    "velocity = torch.zeros(order+1)\n",
    "w_vals = []\n",
    "w_vals.append(w_gd.detach().numpy().copy())\n",
    "for n in range(n_iter):\n",
    "    y = sum(w_gd[i] * x**i for i in torch.arange(order+1))\n",
    "    errors = t-y\n",
    "    loss = torch.sum((errors)**2)/N\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        velocity.mul_(momentum).add_(w_gd.grad, alpha=-alpha)\n",
    "        w_gd.add_(velocity)        \n",
    "        w_gd.grad.zero_()\n",
    "        # velocity = velocity*momentum - alpha*w_gd\n",
    "        # w_gd = torch.tensor(w_gd + velocity, requires_grad=True)\n",
    "    w_vals.append(w_gd.detach().numpy().copy())\n",
    "    \n",
    "# examine solution\n",
    "print('Final guesses: ' + str(w_gd))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.subplot(1, 1,1)\n",
    "xx = np.arange(-10,10,0.1)\n",
    "curr_fn = np.zeros_like(xx)\n",
    "for k in range(xx.shape[0]):\n",
    "    curr_fn[k] = sum(w_gd[a] * xx[k]**a for a in np.arange(order+1))\n",
    "plt.plot(xx, curr_fn, color='blue')\n",
    "plt.scatter(x.detach().numpy(), t, color='orange')\n",
    "plt.grid(True)\n",
    "plt.title('Regressed function: w_0'.format(i))\n",
    "\n",
    "# iter_num = np.array([0, 20000, 40000, 100000, 200000]).astype(int)\n",
    "# plt.figure(figsize=(20, 5))\n",
    "# for j, i in enumerate(iter_num):\n",
    "#     plt.subplot(1, 5, j+1)\n",
    "#     curr_fn = w_vals[i]*x + b_vals[i]\n",
    "#     plt.plot(x.detach().numpy(), curr_fn.detach().numpy(), color='blue')\n",
    "#     plt.scatter(x.detach().numpy(), t, color='orange')\n",
    "#     plt.grid(True)\n",
    "#     plt.title('Regressed function: Iteration {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "iter_num = np.array([0, 5000, 20000, 50000, 200000]).astype(int)\n",
    "plt.figure(figsize=(20, 5))\n",
    "for j, i in enumerate(iter_num):\n",
    "    plt.subplot(1, 5, j+1)\n",
    "    xx = np.arange(-10,10,0.1)\n",
    "    curr_fn = np.zeros_like(xx)\n",
    "    print(w_vals[i])\n",
    "    for k in range(xx.shape[0]):\n",
    "        curr_fn[k] = sum(w_vals[i][a] * xx[k]**a for a in np.arange(order+1))\n",
    "    plt.plot(xx, curr_fn, color='blue')\n",
    "    plt.scatter(x.detach().numpy(), t, color='orange')\n",
    "    plt.grid(True)\n",
    "    plt.title('Regressed function: Iteration {}'.format(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Matrix solution\n",
    "\n",
    "What's interesting is that the exact same logic we used to get the matrix solution for linear regression last lecture also works here! We just got to define a few things: \n",
    "\n",
    "$$\n",
    "y = w_0 + w_1 x + w_2 x^2 + \\cdots + w_n x^m\n",
    "$$\n",
    "\n",
    "In this equation:\n",
    "\n",
    "- \\( y \\) is the dependent variable.\n",
    "- \\( x \\) is the independent variable.\n",
    "- $( w_0, w_1, w_2, \\ldots, w_n )$ are the coefficients of the regression model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we can redefine the system: \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "y_3 \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 & x_1 & x_1^2 & \\cdots & x_1^m \\\\\n",
    "1 & x_2 & x_2^2 & \\cdots & x_2^m \\\\\n",
    "1 & x_3 & x_3^2 & \\cdots & x_3^m \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_n & x_n^2 & \\cdots & x_n^m\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "\\vdots \\\\\n",
    "w_m\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "which can then be written as \n",
    "\n",
    "$$ \n",
    "Y = XW \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So how does this help us? Remember what we did last lecture. We had the loss: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L} &= \\arg\\min_{W} \\frac{1}{2} \\sum_{i=1}^{N} \\left( t^{(i)} - y^{(i)} \\right)^2\\\\ \n",
    "\\mathcal{L} &= \\arg\\min_{W} \\frac{1}{2} \\sum_{i=1}^{N} \\left( t^{(i)} - X^{(i)} W  \\right)^2 \\\\\n",
    "\\mathcal{L} &= \\arg\\min_{W} \\frac{1}{2} \\left( T - X^T W  \\right)^2 \\\\\n",
    "\\mathcal{L} &= \\arg\\min_{W} \\frac{1}{2} \\left( T - X^T W  \\right)^T \\left( T - X^T W  \\right) \\\\\n",
    "\\mathcal{L} &= \\arg\\min_{W} \\frac{1}{2} \\left( T^T T - T^T X^T W - W^T X T + W^T X X^T W \\right) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "to find the minimum we take the derivative of the loss and set it equal to zero: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial W} = 0 &= 0 - T^T X^T - X T + 2 X X^T W \\\\\n",
    "                                                &= - X T - X T + 2 X X^T W \\\\\n",
    "                                             2XT &= 2 X X^T W \\\\\n",
    "                                             X X^T W &= XT \\\\\n",
    "                                             W &= \\left(X X^T\\right)^{-1} X T\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "this means that using the exact same logic as last lecture, we get: \n",
    "\n",
    "$$\n",
    "W = \\left(X^TX\\right)^{-1}X^T T\n",
    "$$\n",
    "\n",
    "(Both formulations are correct, just depends on how you define the initial loss function. Thought I'd show an alternate formulation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def polynomial_to_string(coefficients):\n",
    "    \"\"\"\n",
    "    Converts a list of coefficients into a polynomial equation string with coefficients\n",
    "    formatted to two decimal places.\n",
    "\n",
    "    Parameters:\n",
    "    coefficients (list or array): Coefficients of the polynomial, where the index represents the power of x.\n",
    "\n",
    "    Returns:\n",
    "    str: A string representation of the polynomial equation.\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    degree = len(coefficients) - 1\n",
    "\n",
    "    for power, coeff in enumerate(coefficients):\n",
    "        if coeff == 0:\n",
    "            continue  # Skip zero coefficients\n",
    "\n",
    "        # Determine the sign\n",
    "        sign = '+' if coeff > 0 else '-'\n",
    "\n",
    "        # Format the coefficient to two decimal places\n",
    "        abs_coeff = abs(coeff)\n",
    "        if abs_coeff == 1 and power != 0:\n",
    "            coeff_str = ''\n",
    "        else:\n",
    "            coeff_str = f'{abs_coeff:.2f}'\n",
    "\n",
    "        # Format the variable part\n",
    "        if power == 0:\n",
    "            variable_str = ''\n",
    "        elif power == 1:\n",
    "            variable_str = 'x'\n",
    "        else:\n",
    "            variable_str = f'x^{power}'\n",
    "\n",
    "        # Combine parts\n",
    "        if terms:\n",
    "            term = f' {sign} {coeff_str}{variable_str}'\n",
    "        else:\n",
    "            term = f'{coeff_str}{variable_str}' if sign == '+' else f'-{coeff_str}{variable_str}'\n",
    "\n",
    "        terms.append(term)\n",
    "\n",
    "    # Join all terms and handle the case where all coefficients are zero\n",
    "    polynomial = ''.join(terms) if terms else '0'\n",
    "    return f'y = {polynomial}'\n",
    "\n",
    "# Example usage:\n",
    "coefficients = [2, -4.5678, 0, 3.14159]  # Represents 2 - 4.57x + 3.14x^3\n",
    "equation = polynomial_to_string(coefficients)\n",
    "print(equation)  # Output: y = 2.00 - 4.57x + 3.14x^3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dataset we are artificially creating: \n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "N=10\n",
    "# Generate 10 x values between 0 and 10\n",
    "x = torch.linspace(-10, 10, N)\n",
    "\n",
    "# Generate y values with some variability\n",
    "def f(x): \n",
    "    return 0.25 * x **3 + -0.5 * x**2 + -10 * x + 20 \n",
    "\n",
    "t = torch.clone(f(x) + np.random.normal(0, 15, size=len(x)))\n",
    "\n",
    "#Set polynomial order (2 means we want a fitting function w3 x^3 + w2 x^2 + w1 x + w0)\n",
    "order = 3\n",
    "\n",
    "X = np.ones(N)\n",
    "for i in range(order):\n",
    "    XC = x**(i+1)\n",
    "    X = np.column_stack((X, XC))\n",
    "    \n",
    "w = np.matmul(np.matmul(np.linalg.inv(np.matmul(X.transpose(),X)), X.transpose()), t)\n",
    "print(w)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.subplot(1, 1,1)\n",
    "xx = np.arange(-10,10,0.1)\n",
    "curr_fn = np.zeros_like(xx)\n",
    "for k in range(xx.shape[0]):\n",
    "    curr_fn[k] = sum(w[a] * xx[k]**a for a in np.arange(order+1))\n",
    "plt.plot(xx, curr_fn, color='blue')\n",
    "plt.scatter(x.detach().numpy(), t, color='orange')\n",
    "plt.grid(True)\n",
    "plt.title(polynomial_to_string(w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Beware of overfitting\n",
    "\n",
    "With this new ability, it is tempting to just go to the highest degree polynomial that looks good, but ask yourself, is the model representative of the underlying model?\n",
    "\n",
    "<img align=\"middle\" src=\"./img/poly_fit_9.png\" width=\"500px\" style=\"padding:30px;border:thin solid white;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 10 x values between 0 and 10\n",
    "x = np.linspace(-10, 10, 10)\n",
    "\n",
    "# Generate y values with some variability\n",
    "def f(x): \n",
    "    return 0.25 * x **3 + -0.5 * x**2 + -10 * x + 20 \n",
    "\n",
    "y = f(x) + np.random.normal(0, 15, size=len(x))\n",
    "\n",
    "\n",
    "# Create the figure and axis\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Scatter plot of generated points\n",
    "plt.scatter(x, y, label='Generated Data', color='blue')\n",
    "\n",
    "# Plot the line\n",
    "px = np.linspace(-10, 10, 100)\n",
    "# plt.plot(px,f(px), label='y = 2x + 5', color='blue', linewidth=2)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "#plt.ylim(0, 30)\n",
    "#plt.title('Scatter Plot with Best Fit Line (Seed = 42)')\n",
    "#plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig(\"./img/scatter_plot_with_sample_polynomial_no_lines.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization \n",
    "\n",
    "An issue with regularization? noise in data can skew certain parameters:  \n",
    "\n",
    "<img align=\"middle\" src=\"./img/reg_example_lambda=0.png\" width=\"500px\" style=\"padding:30px;border:thin solid white;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's assume we know all the parameters are roughly the same order of magnitude. Maybe we can encourage the model to normalize the parameters to minimize one parameter getting pulled a particular way? \n",
    "\n",
    "We can what's called a **L2 penalty term** to the loss function, and this is called **L2 regularization**.:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w) = \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2 + \\lambda \\sum_{j=0}^{d} w_j^2\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is a fitting term. This is called L2 penalty just because itâ€™s a L2-norm of \\( w \\). \n",
    "\n",
    "<img align=\"middle\" src=\"./img/losses_surf.png\" width=\"800px\" style=\"padding:30px;border:thin solid white;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In fancy terms, this whole loss function is also known as **Ridge regression**.\n",
    "\n",
    "<img align=\"middle\" src=\"./img/losses_contours.png\" width=\"800px\" style=\"padding:30px;border:thin solid white;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Interestingly, the same matrix solution we've developed for the matrix solution can be modified to include L2 regularization: \n",
    "\n",
    "\"First write the loss function in matrix notation:\n",
    "\n",
    "$$\n",
    "L(w) = \\|y - Xw\\|^2 + \\lambda \\|w\\|^2_2\n",
    "$$\n",
    "\n",
    "Then the gradient is:\n",
    "\n",
    "$$\n",
    "\\nabla L_w = -2X^T(y - Xw) + 2\\lambda w\n",
    "$$\n",
    "\n",
    "Setting to zero and solving:\n",
    "\n",
    "$$\n",
    "0 = -2X^T(y - Xw) + 2\\lambda w\n",
    "$$\n",
    "\n",
    "$$\n",
    "= X^T(y - Xw) - \\lambda w\n",
    "$$\n",
    "\n",
    "$$\n",
    "= X^T y - X^T X w - \\lambda w\n",
    "$$\n",
    "\n",
    "$$\n",
    "= X^T y - (X^T X + \\lambda I_d) w\n",
    "$$\n",
    "\n",
    "Move that to the other side and we get a closed-form solution:\n",
    "\n",
    "$$\n",
    "(X^T X + \\lambda I_d) w = X^T y\n",
    "$$\n",
    "\n",
    "$$\n",
    "W = (X^T X + \\lambda I_d)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "which is almost the same as linear regression without regularization.\" [2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dataset we are artificially creating: \n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "N=10\n",
    "# Generate 10 x values between 0 and 10\n",
    "xlims = [-10, 10]\n",
    "x = torch.linspace(xlims[0], xlims[1], N)\n",
    "\n",
    "# Generate y values with some variability\n",
    "def f(x): \n",
    "    return -1 * x + 1\n",
    "\n",
    "random_spikes = np.zeros(N)\n",
    "random_spikes[1] = 35\n",
    "random_spikes[7] = 55\n",
    "t = f(x) + np.random.normal(0, 0.5, size=len(x)) + random_spikes\n",
    "\n",
    "#Set polynomial order (2 means we want a fitting function w3 x^3 + w2 x^2 + w1 x + w0)\n",
    "order = 1\n",
    "\n",
    "X = np.ones(N)\n",
    "for i in range(order):\n",
    "    XC = x**(i+1)\n",
    "    X = np.column_stack((X, XC))\n",
    "\n",
    "lambdaa = 50\n",
    "w = np.matmul(np.matmul(np.linalg.inv(np.matmul(X.transpose(),X) + lambdaa*np.identity(order+1)), X.transpose()), t)\n",
    "print(w)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.subplot(1, 1,1)\n",
    "xx = np.arange(min(x), max(x), 0.1)\n",
    "curr_fn = np.zeros_like(xx)\n",
    "for k in range(xx.shape[0]):\n",
    "    curr_fn[k] = sum(w[a] * xx[k]**a for a in np.arange(order+1))\n",
    "plt.plot(xx, curr_fn, color='blue')\n",
    "plt.scatter(x, t, color='orange')\n",
    "plt.grid(True)\n",
    "plt.title(polynomial_to_string(w))\n",
    "\n",
    "plt.savefig('./img/reg_example_lambda='+str(lambdaa))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Increasing lambda suppresses parameter values. Can be a gift or a curse  depending on your preprocessing, data, and models: \n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around; text-align: center;\">\n",
    "    <div>\n",
    "        <p>$\\lambda = 0$</p>\n",
    "        <img src=\"./img/reg_example_lambda=0.png\" alt=\"Image 1\" width=\"300\">\n",
    "    </div>\n",
    "    <div>\n",
    "        <p>$\\lambda = 50$</p>\n",
    "        <img src=\"./img/reg_example_lambda=50.png\" alt=\"Image 2\" width=\"300\">\n",
    "    </div>\n",
    "    <div>\n",
    "        <p>$\\lambda = 1000$</p>\n",
    "        <img src=\"./img/reg_example_lambda=1000.png\" alt=\"Image 3\" width=\"300\">\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic dataset\n",
    "N = 10\n",
    "x = np.linspace(-10, 10, N)\n",
    "\n",
    "def f(x): \n",
    "    return -1 * x + 2 \n",
    "\n",
    "random_spikes = np.zeros(N)\n",
    "random_spikes[8] = 10  \n",
    "t = f(x) + np.random.normal(0, 2.0, size=len(x)) + random_spikes\n",
    "\n",
    "# Define grid of m0 and m1 values\n",
    "m0_vals = np.linspace(-5, 5, 30)  # Reduced resolution for efficiency\n",
    "m1_vals = np.linspace(-3, 3, 30)\n",
    "M0, M1 = np.meshgrid(m0_vals, m1_vals)\n",
    "\n",
    "# Compute loss for each (m0, m1) pair efficiently\n",
    "losses = np.zeros_like(M0)\n",
    "L2_losses = np.zeros_like(M0)\n",
    "\n",
    "lambda_reg = 1  # Regularization strength\n",
    "\n",
    "for i in range(M0.shape[0]):\n",
    "    for j in range(M0.shape[1]):\n",
    "        y_pred = M1[i, j] * x + M0[i, j]\n",
    "        loss = np.mean((y_pred - t) ** 2) + lambda_reg * (M0[i, j]**2 + M1[i, j]**2)\n",
    "        losses[i, j] = np.mean((y_pred - t) ** 2)\n",
    "        L2_losses[i, j] = (M0[i, j]**2 + M1[i, j]**2)\n",
    "\n",
    "# Plot 3D surface\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot standard least squares loss\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(M0, M1, losses, cmap='coolwarm', alpha=0.7)\n",
    "ax1.set_xlabel('w[0]')\n",
    "ax1.set_ylabel('w[1]')\n",
    "ax1.set_title('Linear Regression losses')\n",
    "\n",
    "# Ridge regression plot\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.plot_surface(M0, M1, L2_losses, cmap='coolwarm', alpha=0.7)\n",
    "ax2.set_xlabel('w[0]')\n",
    "ax2.set_ylabel('w[1]')\n",
    "ax2.set_title('L2 losses')\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"./img/losses_surf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic dataset\n",
    "N = 10\n",
    "x = np.linspace(-10, 10, N)\n",
    "\n",
    "def f(x): \n",
    "    return -1 * x + 2 \n",
    "\n",
    "random_spikes = np.zeros(N)\n",
    "random_spikes[8] = 10  \n",
    "t = f(x) + np.random.normal(0, 2.0, size=len(x)) + random_spikes\n",
    "\n",
    "# Define grid of m0 and m1 values\n",
    "m0_vals = np.linspace(-5, 5, 100)  # Wider range for better visualization\n",
    "m1_vals = np.linspace(-5, 5, 100)\n",
    "M0, M1 = np.meshgrid(m0_vals, m1_vals)\n",
    "\n",
    "# Compute loss for each (m0, m1) pair efficiently\n",
    "losses = np.zeros_like(M0)\n",
    "L2_losses = np.zeros_like(M0)\n",
    "\n",
    "lambda_reg = 1  # Regularization strength\n",
    "\n",
    "for i in range(M0.shape[0]):\n",
    "    for j in range(M0.shape[1]):\n",
    "        y_pred = M1[i, j] * x + M0[i, j]\n",
    "        losses[i, j] = np.mean((y_pred - t) ** 2)\n",
    "        L2_losses[i, j] = (M0[i, j]**2 + M1[i, j]**2)\n",
    "\n",
    "# Find min points\n",
    "min_idx = np.unravel_index(np.argmin(losses), losses.shape)\n",
    "min_m0, min_m1 = M0[min_idx], M1[min_idx]\n",
    "\n",
    "ridge_m0 = min_m0 / (1 + lambda_reg)\n",
    "ridge_m1 = min_m1 / (1 + lambda_reg)\n",
    "\n",
    "# Create Contour Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot MSE Loss Contours (Red)\n",
    "contour1 = ax.contour(M0, M1, losses, levels=15, cmap=\"Reds\", alpha=0.8)\n",
    "ax.scatter(min_m0, min_m1, color='red', label=f'Linear ({int(min_m0)},{int(min_m1)})')\n",
    "\n",
    "# Plot L2 Regularization Loss Contours (Blue)\n",
    "contour2 = ax.contour(M0, M1, L2_losses, levels=10, cmap=\"Blues\", alpha=0.8)\n",
    "ax.scatter(0, 0, color='blue', label=\"L2\", marker='o', s=80)\n",
    "\n",
    "# Ridge Regression Min Point\n",
    "ax.scatter(ridge_m0, ridge_m1, color='green', label=f'Ridge ({int(ridge_m0)},{int(ridge_m1)})', s=80)\n",
    "\n",
    "# Labels and Formatting\n",
    "ax.axhline(0, color='black', linewidth=1)  # x-axis\n",
    "ax.axvline(0, color='black', linewidth=1)  # y-axis\n",
    "ax.set_xlabel(\"w[0]\")\n",
    "ax.set_ylabel(\"w[1]\")\n",
    "ax.set_title(f\"Linear Regression and L2 loss contours (Lambda={lambda_reg})\")\n",
    "\n",
    "# Annotate\n",
    "ax.text(min_m0, min_m1, \" Linear\", color='red', fontsize=12)\n",
    "ax.text(ridge_m0, ridge_m1, \" Ridge\", color='green', fontsize=12)\n",
    "ax.text(0, 0, \" L2\", color='blue', fontsize=12)\n",
    "\n",
    "# Legend\n",
    "ax.legend()\n",
    "\n",
    "# Save and Show\n",
    "#plt.savefig(\"./img/losses_contours.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## That's it for today\n",
    "\n",
    "Next time we'll talk about the limits of linear regression and more about logistic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## Additional References\n",
    "\n",
    "[1] Roger Grosse CSC321 lectures - https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/\n",
    "[2] Freindly ML Tutorial - Linear regression with regularization -  https://aunnnn.github.io/ml-tutorial/html/blog_content/linear_regression/linear_regression_regularized.html"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
