{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <img align=\"left\" src=\"img/ECE364-logo.png\" width=\"300px\" style=\"padding:30px;border:thin solid white;\"> \n",
    "\n",
    "# Lecture 10 - Multi-class logistic regression\n",
    "## ECE364 - Programming Methods for Machine Learning\n",
    "### Nickvash Kani \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### Slides based off prior lectures by Alex Schwing, Aigou Han, Farzad Kamalabadi, Corey Snyder. All mistakes are my own!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Things we'll discuss in this lecture: \n",
    "\n",
    "- Binary cost functions\n",
    "    - Linear \n",
    "    - Logistic squared-error\n",
    "    - Logistic cross-entropy\n",
    "- Multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Binary loss(cost) functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's review the regression models we've investigated so far. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Some definitions: \n",
    "\n",
    "- Dataset ${\\cal D} = \\{(x^{(i)}, y^{(i)}\\}$\n",
    "- Labels: $t^{(i)}\\in\\{0, 1, \\dots, C-1\\}$. \n",
    "     - For binary classification, there are only two labels ($t^{(i)} \\in \\{ 0, 1\\}$)\n",
    "\n",
    "How can we learn a classifier to deal with this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Regression\n",
    "\n",
    "Linear regression is the simplest model to fit (and even has exact solution(s)!). Can we use that: \n",
    "\n",
    "$$\n",
    "y = \\mathbf{w}^\\mathsf{T} \\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{SE}(y, t) = \\tfrac{1}{2} (y - t)^2\n",
    "$$\n",
    "\n",
    "- Doesn't matter that the targets are actually binary.\n",
    "- Threshold predictions at \\(y = 1/2\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "#### The problem \n",
    "\n",
    "<img align=\"center\" src=\"img/lin_reg_problem.png\" width=\"600px\" style=\"padding:30px;border:thin solid white;\"> \n",
    "\n",
    "linear regression hates when a prediction is made with a high degree of confidence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic Function \n",
    "\n",
    "- No reason to predict values outside [0, 1]. Let's squash $y$ into this interval.\n",
    "\n",
    "- The logistic function is a kind of sigmoidal, or S-shaped, function:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "- A linear model with a logistic nonlinearity is known as **log-linear**:\n",
    "\n",
    "$$\n",
    "z = \\mathbf{w}^\\mathsf{T}\\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = \\sigma(z)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{SE}(y, t) = \\tfrac{1}{2} (y - t)^2\n",
    "$$\n",
    "\n",
    "- Used in this way, $\\sigma$ is called an **activation function**, and $z$ is called the **logit**.\n",
    "\n",
    "[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### The problem\n",
    "\n",
    "<img align=\"center\" src=\"img/log_mse_loss_plot.png\" width=\"800px\" style=\"padding:30px;border:thin solid white;\"> \n",
    "\n",
    "- In gradient descent, a small gradient implies a small step should be taken.\n",
    "\n",
    "- But if the prediction is very wrong, shouldn't a large step be taken? \n",
    "\n",
    "- We say the learning algorithm does not have a strong **gradient signal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Logistic (sigmoid) function\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "# Squared-error loss for one sample with label t=1:\n",
    "# L(z) = 0.5 * (sigmoid(z) - 1)^2\n",
    "def squared_error_loss(z, t=1.0):\n",
    "    return 0.5 * (sigmoid(z) - t)**2\n",
    "\n",
    "# Derivative of L(z) w.r.t. z\n",
    "# dL/dz = (sigmoid(z) - t) * sigmoid(z) * (1 - sigmoid(z))\n",
    "def dL_dz(z, t=1.0):\n",
    "    s = sigmoid(z)\n",
    "    return (s - t) * s * (1 - s)\n",
    "\n",
    "# Create a range of z-values\n",
    "z_values = np.linspace(-10, 5, 400)\n",
    "\n",
    "# Compute the squared-error loss for each z\n",
    "L_values = squared_error_loss(z_values)\n",
    "\n",
    "# Plot the squared-error loss\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(z_values, L_values, 'b', label='Squared Error Loss')\n",
    "\n",
    "# Mark vertical line at z=0 for reference (optional)\n",
    "plt.axvline(x=0, color='gray', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot tangent lines at z = -6, -4, -2\n",
    "for z0 in [-6, -4, -2]:\n",
    "    L0 = squared_error_loss(z0)\n",
    "    slope = dL_dz(z0)\n",
    "    \n",
    "    # We'll plot the tangent line in some neighborhood around z0\n",
    "    z_tangent = np.linspace(z0 - 2, z0 + 2, 50)\n",
    "    L_tangent = L0 + slope * (z_tangent - z0)\n",
    "    \n",
    "    plt.plot(z_tangent, L_tangent, 'r', alpha=0.8)\n",
    "    plt.scatter(z0, L0, color='red')  # highlight the point of tangency\n",
    "\n",
    "# Set axis limits and labels\n",
    "plt.xlim([-10, 5])\n",
    "plt.ylim([-0.1, 0.6])\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Squared-Error Loss for Logistic Function (t=1)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "#plt.show()\n",
    "plt.savefig(\"img/log_mse_loss_plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross-entropy loss\n",
    "\n",
    "- Because $y \\in [0, 1]$, we can interpret it as the estimated probability that $t = 1$.\n",
    "\n",
    "- The pundits who were 99% confident Clinton would win were much more wrong than the ones who were only 90% confident.\n",
    "\n",
    "- **Cross-entropy loss** captures this intuition:\n",
    "\n",
    "$$\n",
    "z = w^Tx+b\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    y &= \\sigma(z)\\\\\n",
    "      &= \\frac{1}{1+e^{-z}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{CE}(y, t) = \n",
    "\\begin{cases}\n",
    "-\\log y, & \\text{if } t = 1,\\\\\n",
    "-\\log (1 - y), & \\text{if } t = 0,\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "which can also be written as\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{CE}(y, t) \n",
    "= -\\,t \\,\\log y \\;-\\; (1 - t)\\,\\log\\bigl(1 - y\\bigr).\n",
    "$$\n",
    "\n",
    "\n",
    "<img align=\"center\" src=\"img/log_ce_loss_plot.png\" width=\"600px\" style=\"padding:30px;border:thin solid white;\"> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Cross-entropy loss for a single sample, label t in {0,1}:\n",
    "def cross_entropy_loss(z, t):\n",
    "    s = sigmoid(z)\n",
    "    # Add a tiny epsilon to avoid log(0) if desired\n",
    "    epsilon = 1e-15\n",
    "    s = np.clip(s, epsilon, 1 - epsilon)\n",
    "    return - (t * np.log(s) + (1 - t) * np.log(1 - s))\n",
    "\n",
    "# Create a range of z-values\n",
    "z_values = np.linspace(-6, 6, 400)\n",
    "\n",
    "# Compute CE loss for t=1 and t=0\n",
    "L_pos = cross_entropy_loss(z_values, t=1)\n",
    "L_neg = cross_entropy_loss(z_values, t=0)\n",
    "\n",
    "# Plot both on the same figure\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(z_values, L_pos, 'b', label='Cross-Entropy (t=1)')\n",
    "plt.plot(z_values, L_neg, 'r', label='Cross-Entropy (t=0)')\n",
    "\n",
    "# Axis labels, legend, grid, etc.\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Cross-Entropy Logistic Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.ylim([0, 6])  # Adjust as needed\n",
    "#plt.show()\n",
    "plt.savefig(\"img/log_ce_loss_plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-class regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "OK so we've exhausted binary classification. But maany classifications are no binary: \n",
    "\n",
    "- Animal species\n",
    "- Car manufacturers/models \n",
    "- Final grade\n",
    "\n",
    "So what do we do when we have inputs that have more than two classification classes? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reduction to binary classification (one vs rest)\n",
    "\n",
    "If we insist on using binary classifiers, we are not absent options:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img align=\"center\" src=\"img/one_vs_all.png\" width=\"600px\" style=\"padding:30px;border:thin solid white;\"> \n",
    "\n",
    "Strategy: Train three classifiers with $y \\in [0,1]$ where each classifier considers another class as the positive class. \n",
    "\n",
    "We then get three classification models:\n",
    "\n",
    "- $f_1 (x; w)$ for classifying triangles\n",
    "- $f_2 (x; w)$ for classifying x's\n",
    "- $f_3 (x; w)$ for classifying circles\n",
    "\n",
    "Final predictions: $\\arg\\max_k f_k(x,w)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Generate a synthetic dataset with three classes\n",
    "X, y = make_classification(n_samples=300, n_features=2, n_classes=3, n_clusters_per_class=1, n_redundant=0, random_state=42)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Define a simple logistic regression model\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(2, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))  # Sigmoid for binary classification\n",
    "\n",
    "# Train three classifiers (one vs all)\n",
    "models = []\n",
    "for class_idx in range(3):\n",
    "    model = BinaryClassifier()\n",
    "    criterion = nn.BCELoss()  # Binary cross-entropy loss\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    \n",
    "    # Convert labels to binary (1 for the class, 0 for others)\n",
    "    y_binary = (y_train_tensor == class_idx).float().unsqueeze(1)\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(1000):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_binary)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    models.append(model)\n",
    "\n",
    "# Plot the decision boundaries\n",
    "xx, yy = np.meshgrid(np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 100),\n",
    "                     np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, 100))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "grid_tensor = torch.tensor(grid, dtype=torch.float32)\n",
    "\n",
    "# Get predictions from all models\n",
    "predictions = np.zeros((grid.shape[0], 3))\n",
    "for class_idx, model in enumerate(models):\n",
    "    predictions[:, class_idx] = model(grid_tensor).detach().numpy().ravel()\n",
    "\n",
    "# Assign each point to the class with the highest probability\n",
    "final_predictions = np.argmax(predictions, axis=1)\n",
    "final_predictions = final_predictions.reshape(xx.shape)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(xx, yy, final_predictions, alpha=0.3, cmap=plt.cm.coolwarm)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor=\"k\", cmap=plt.cm.coolwarm)\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Multiclass Classification with PyTorch (One-vs-All)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multi-class regression\n",
    "\n",
    "What if there are 1000 classes? Going through a whole bunch of effort to reuse some old binary classification code kinda negates the time savings by using the old code in the first place.... \n",
    "\n",
    "Let's regain some footing. What do we know? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall the sigmoid function is defined as\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}},\n",
    "$$\n",
    "\n",
    "where $z \\in \\mathbb{R}$. The function $\\sigma(z)$ maps any real value $z$ to a number in the interval $(0,1)$ and is a monotonically increasing function.\n",
    "\n",
    "The probability is thus represented by\n",
    "\n",
    "$$\n",
    "P(y \\mid x) =\n",
    "\\begin{cases}\n",
    "\\sigma\\bigl(w^\\mathsf{T} x\\bigr), & \\text{if } y = +1,\\\\\n",
    "1 - \\sigma\\bigl(w^\\mathsf{T} x\\bigr), & \\text{if } y = -1,\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "which can also be written as\n",
    "\n",
    "$$\n",
    "P(y \\mid x) = \\sigma\\bigl(y \\cdot w^\\mathsf{T} x\\bigr),\n",
    "$$\n",
    "\n",
    "due to the fact that $\\theta(-z) = 1 - \\theta(z)$. Note that in the binary case, we only need to estimate one probability, because $P(y = +1 \\mid x)$ and $P(y = -1 \\mid x)$ sum to 1.\n",
    "\n",
    "[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But that is for binary classes. What do we need to satisfy for multiple classes: \n",
    "\n",
    "- non-negativity: $p(Y=y|x) \\geq 0$ $\\forall y\\in\\{0, \\dots, C-1\\}$\n",
    "- sum to one: $\\sum_{y\\in\\{0, \\dots, C-1\\}} p(Y=y|x) = 1$\n",
    "\n",
    "What kind of function can satisfy these constraints? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Well if a single logistic fuction is of the form: \n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{e^z}{1 + e^z} \\;=\\; \\frac{1}{1 + e^{-z}},\n",
    "$$\n",
    "\n",
    "maybe we can add the probabilities together to get:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(\\mathbf{v}) \n",
    "= \n",
    "\\frac{1}{\\sum_{c=0}^{C-1} e^{v_c}}\n",
    "\\begin{bmatrix}\n",
    "e^{v_0} \\\\[6pt]\n",
    "e^{v_1} \\\\[6pt]\n",
    "\\vdots  \\\\[6pt]\n",
    "e^{v_{C-1}}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "This is referred to as the softmax function. Using the softmax function we can define the probability that $x$ belongs to a particular class $c$ as: \n",
    "\n",
    "$$p(Y=t|x) = \\frac{\\exp(w_t^Tx)}{\\sum_{y\\in\\{0, \\dots, C-1\\}} \\exp(w_{y}^Tx)}$$\n",
    "\n",
    "The inputs $w^Tx$ are called **logits**.\n",
    "\n",
    "- Note: every class has its own weight vector $w_y$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's train a model using this probabilistic model and let's study the result to see whether this makes sense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Example data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "x0 = torch.cat([torch.randn((100,))-1,torch.randn((100,))+1,torch.randn((100,))+3],dim=0)\n",
    "y = torch.cat([torch.zeros((100,),dtype=torch.int8),torch.ones((100,),dtype=torch.int8),2*torch.ones((100,),dtype=torch.int8)],dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x0,y,'rx')\n",
    "plt.grid(True)\n",
    "plt.xlabel('x_0')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How did we train a model again?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Maximum-likelihood or minimizing the negative log-likelihood:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ loss(x,t; w) = \\min_w \\sum_{(x^{(i)},t^{(i)})\\in{\\cal D}} -\\log p(t|x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's plug our probability model in there and see what happens:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "loss(x,t; w) &= \\min_w \\sum_{(x^{(i)},t^{(i)})\\in{\\cal D}} -\\log p(t|x)\\\\\n",
    "&= \\min_w \\sum_{(x^{(i)},t^{(i)})\\in{\\cal D}} -\\log\\frac{\\exp(w_t^Tx)}{\\sum_{c\\in\\{0, \\dots, C-1\\}} \\exp(w_{c}^Tx)}\\\\\n",
    "&= \\min_w \\sum_{(x^{(i)},t^{(i)})\\in{\\cal D}} -\\left(\\log \\exp(w_t^Tx) - \\log \\sum_{c\\in\\{0, \\dots, C-1\\}} \\exp(w_{c}^Tx) \\right)\\\\\n",
    "&=\\min_w \\sum_{(x^{(i)},t^{(i)})\\in{\\cal D}} \\left(- w_{t^{(i)}}^Tx + \\log\\sum_{c\\in\\{0, \\dots, C-1\\}} \\exp(w_{c}^Tx)\\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How to efficiently implement this objective function? Let's put the weight vectors for each class into a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$W = \\left[\\begin{array}{ccc}\\vdots&&\\vdots\\\\w_0&\\dots&w_{C-1}\\\\\\vdots&&\\vdots\\end{array}\\right]\\in\\mathbb{R}^{D\\times C}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "W0 = torch.randn((2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = torch.cat([x0.unsqueeze(0), torch.ones_like(x0).unsqueeze(0)],dim=0)\n",
    "def objfun(X,y,W):\n",
    "    #res1 = (W[:,y.to(torch.long)]*X).sum(dim=0)\n",
    "    logits = W.t()@X\n",
    "    firstterm = -torch.gather(logits,dim=0,index=y.view(1,-1).to(torch.int64))\n",
    "    secondterm = torch.logsumexp(logits,dim=0,keepdim=True)\n",
    "    return torch.mean(firstterm+secondterm)\n",
    "objfun(X,y,W0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "w = W0.clone()\n",
    "w.requires_grad = True\n",
    "alpha = 1.0\n",
    "numIter = 200\n",
    "funs = torch.zeros((numIter,1))\n",
    "optimizer = torch.optim.SGD([w],lr=alpha,momentum=0,dampening=0,weight_decay=0)\n",
    "for iter in range(numIter):\n",
    "    optimizer.zero_grad()\n",
    "    f = objfun(X,y,w)\n",
    "    f.backward()\n",
    "    optimizer.step()\n",
    "    funs[iter] = f.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(range(numIter),funs)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What's our $W$ matrix now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How do we check that this makes sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that we can't plot the loss function in parameter space anymore. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have 6 parameters, i.e., 6-dimensional space. Hard to plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But we can plot the probabilities: $$p(Y=t|x) = \\frac{\\exp(w_t^Tx)}{\\sum_{y\\in\\{0, \\dots, C-1\\}} \\exp(w_{y}^Tx)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xaxis = torch.linspace(-5,5,100)\n",
    "Xxaxis = torch.cat([xaxis.unsqueeze(0),torch.ones_like(xaxis).unsqueeze(0)],dim=0)\n",
    "logits = w.detach().t()@Xxaxis\n",
    "probs = torch.nn.functional.softmax(logits,dim=0)\n",
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(xaxis,probs.t())\n",
    "plt.grid(True)\n",
    "plt.xlabel('x_0')\n",
    "plt.ylabel('p(y|x)')\n",
    "plt.legend(['C = 0','C = 1','C = 2'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Seems reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Can we avoid some of the code above? I.e., can we compress this more and use more pytorch code?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Pytorch's ```torch.CrossEntropyLoss``` (https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) looks very similar to what we need:\n",
    "\n",
    "The loss can be described as:\n",
    "\n",
    "$$\n",
    "\\text{loss}(x, \\text{class}) \n",
    "= \n",
    "-\\log \\!\\Bigl(\\frac{\\exp\\bigl(x[\\text{class}]\\bigr)}{\\sum_j \\exp\\bigl(x[j]\\bigr)}\\Bigr).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How can we use this one? What do we need?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def LinearNetFun(X,W):\n",
    "    logits = W.t()@X\n",
    "    return logits.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "w = W0.clone()\n",
    "w.requires_grad = True\n",
    "alpha = 1.0\n",
    "numIter = 200\n",
    "funs = torch.zeros((numIter,1))\n",
    "optimizer = torch.optim.SGD([w],lr=alpha,momentum=0,dampening=0,weight_decay=0)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "ylong = y.to(torch.long)\n",
    "for iter in range(numIter):\n",
    "    optimizer.zero_grad()\n",
    "    f = loss(LinearNetFun(X,w),ylong)\n",
    "    f.backward()\n",
    "    optimizer.step()\n",
    "    funs[iter] = f.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Same result as before. Very nice. Looks like we used ```torch.CrossEntropyLoss``` the right way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Weights shouldn't be defined anywhere in the code. Let's clean this up a little more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall the plotting of composite functions from Lecture 4. Let's use the same syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class OurNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OurNet,self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(2,3,bias=False)\n",
    "    def forward(self,x):\n",
    "        return self.fc1(x.t())\n",
    "net1 = OurNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "net1.fc1.weight.data = W0.t().data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 1.0\n",
    "numIter = 200\n",
    "funs = torch.zeros((numIter,1))\n",
    "optimizer = torch.optim.SGD(net1.parameters(),lr=alpha,momentum=0,dampening=0,weight_decay=0)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "ylong = y.to(torch.long)\n",
    "for iter in range(numIter):\n",
    "    optimizer.zero_grad()\n",
    "    f = loss(net1(X),ylong)\n",
    "    f.backward()\n",
    "    optimizer.step()\n",
    "    funs[iter] = f.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(net1.fc1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Same result again. Very nice, this is starting to look clean now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But wait, why do we manually append our data with ones? Isn't that wasteful? Can we avoid this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class OurNet2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OurNet2,self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(1,3,bias=True)\n",
    "    def forward(self,x):\n",
    "        return self.fc1(x)\n",
    "net2 = OurNet2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "net2.fc1.weight.data = W0[0,:].view(-1,1).data.clone()\n",
    "net2.fc1.bias.data = W0[1,:].view(-1).data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 1.0\n",
    "numIter = 200\n",
    "funs = torch.zeros((numIter,1))\n",
    "optimizer = torch.optim.SGD(net2.parameters(),lr=alpha,momentum=0,dampening=0,weight_decay=0)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "ylong = y.to(torch.long)\n",
    "for iter in range(numIter):\n",
    "    optimizer.zero_grad()\n",
    "    f = loss(net2(x0.view(-1,1)),ylong)\n",
    "    f.backward()\n",
    "    optimizer.step()\n",
    "    funs[iter] = f.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(net2.fc1.weight)\n",
    "print(net2.fc1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## That's it for today\n",
    "\n",
    "- Next time we'll explore the torch.nn library. \n",
    "    - It will make our lives easier when we go onto more advanced neural networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "[1] Roger Grosse CSC321 lectures - https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/\n",
    "\n",
    "[2] Shuiwang Ji Notes, \"Logistic Regression: From Binary to Multi-class - https://people.tamu.edu/~sji/classes/LR.pdf"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
