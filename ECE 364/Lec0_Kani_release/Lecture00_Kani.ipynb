{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "cell_style": "center",
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img align=\"left\" src=\"img/ECE364-logo.png\" width=\"300px\" style=\"padding:30px;border:thin solid white;\"> \n",
    "\n",
    "# Lecture 0 - Course Intro and PyTorch Basics\n",
    "## ECE364 - Programming Methods for Machine Learning\n",
    "### Nickvash Kani \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### Slides based off prior lectures by Alex Schwing, Aigou Han, Farzas Kamalabadi, Corey Snyder. All mistakes are my own!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Topics covered: \n",
    "\n",
    "- Learning machine learning fundamentals including: \n",
    "    - linear algebra review\n",
    "    - automatic regression and classifcation fitting models\n",
    "    - building complex neural networks (deep models)\n",
    "\n",
    "- Learn the fundamentals of PyTorch for basic deep learning models\n",
    "    - efficient computation/storage\n",
    "    - computational graphs/back-propagation\n",
    "    - formatting data and training NN models\n",
    "    \n",
    "- Advanced machine learning topics\n",
    "    - advanced deep network concepts like attention (transformer models)\n",
    "    - large language models basics and usage\n",
    "\n",
    "**Big goal:** learn machine learning basics through applications. (Theory is awesome, but most of us just want to passs a interview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "View course website and show the following: \n",
    "- Course staff\n",
    "- Course structure\n",
    "- Homeworks\n",
    "- Exams\n",
    "- Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lecture plan:\n",
    "\n",
    "- Introduction to PyTorch \n",
    "- Sample PyTorch Code \n",
    "- Tensor Basics\n",
    "- Tensor Operations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1:  Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is PyTorch? \n",
    "\n",
    "PyTorch is a open-source machine learning library used for various tasks that require some level of model optimization like natural language processing, computer vision. \n",
    "\n",
    "- Originally developed by Meta AI and now a part of the Linux Foundation \n",
    "- Developed in conjunction with with Convolutional Architecture for Fast Feature Encoding (Caffe2) (C++  machine learning framework) \n",
    "    - But models between frameworks were incompatible ...\n",
    "    - So Meta and Microsoft created the Open Neural Network Exchange (ONNX) for converting ML models between frameworks\n",
    "    - Caffe2 was merged into PyTorch 2 years later \n",
    "- PyTorch 2 was released in 2023 and introduced TorchDynamo, a Python level compiler that offered a lot of ML specific optimizations and significant speed-ups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why use PyTorch\n",
    "\n",
    "When PyTorch was introduced in 2017, Tensorflow and Keras (Tersorflow wrapper) were already mature machine learning libraries. But now 85% of academic papers use PyTorch: **why?**\n",
    "\n",
    "- Simplicity - easy to use, **extend** and debug\n",
    "- Python integration - Python is now the most popular language in the world\n",
    "- The tensor data type, perfect abstraction of NumPy data\n",
    "- Accelerated computation using GPUs\n",
    "- Early Caffe2 integration gave it a high performing C++ compiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 2: Feeling out PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Software installation and initialization\n",
    "\n",
    "[Install PyTorch using anaconda or pip](https://pytorch.org/get-started/locally/)\n",
    "\n",
    "Check your successful installation using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Datasets\n",
    "\n",
    "Many classic datasets come [preprogrammed into PyTorch](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "\n",
    "Let's look at one of these datasets: [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "\n",
    "*Due to licensing issues CIFAR is no longer a default in the PyTorch library but it is still easily accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#credit to https://jamesmccaffrey.wordpress.com/2020/08/07/displaying-cifar-10-images-using-pytorch/\n",
    "\n",
    "import torch as T\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "  img = img / 2 + 0.5   # unnormalize\n",
    "  npimg = img.numpy()   # convert from tensor\n",
    "  plt.imshow(np.transpose(npimg, (1, 2, 0))) \n",
    "  plt.axis('off')\n",
    "  plt.show()\n",
    "\n",
    "transform = transforms.Compose( [transforms.ToTensor(),\n",
    "transforms.Normalize((0.5, 0.5, 0.5),\n",
    "  (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = tv.datasets.CIFAR10(root='.\\\\data', train=True, download=True, transform=transform)\n",
    "trainloader = T.utils.data.DataLoader(trainset,\n",
    "batch_size=100, shuffle=False, num_workers=1)\n",
    "\n",
    "# classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog',\n",
    "#   'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# get first 100 training images\n",
    "dataiter = iter(trainloader)\n",
    "imgs, lbls = next(dataiter)\n",
    "\n",
    "images = []\n",
    "for i in range(100):  # show just the cars\n",
    "    if lbls[i] == 1:  # 1 = car\n",
    "        images.append(imgs[i])\n",
    "\n",
    "images_grid = tv.utils.make_grid(images, nrow=10)        \n",
    "imshow(images_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Small Torch Model\n",
    "\n",
    "[AlexNet](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf): \n",
    "\n",
    "<img align=\"left\" src=\"img/alexnet.jpg\" width=\"1000px\" style=\"padding:30px;border:thin solid white;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Jupyter notebook demonstration (on Google colab)\n",
    "\n",
    "- http://colab.research.google.com (provides access to GPUs and TPUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 3: Introduction to Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's think about vectors and how we would initialize/use them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vec1 = [1,2,3]\n",
    "\n",
    "print(vec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vec2 = np.array(vec1)\n",
    "print(vec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why do we use numpy if vanilla python has arrays/matrices anyway? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "size_of_vec = int(1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def pure_python_version():\n",
    "    t1 = time.time()\n",
    "    X = range(size_of_vec)\n",
    "    Y = range(size_of_vec)\n",
    "    Z = [X[i] + Y[i] for i in range(len(X)) ]\n",
    "    return time.time() - t1\n",
    "\n",
    "t1 = pure_python_version()\n",
    "print('Pure Python time: {:.6f}'.format(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def numpy_version():\n",
    "    t1 = time.time()\n",
    "    X = np.arange(size_of_vec)\n",
    "    Y = np.arange(size_of_vec)\n",
    "    Z = X + Y\n",
    "    return time.time() - t1\n",
    "\n",
    "t2 = numpy_version()\n",
    "print('Numpy time: {:.6f}'.format(t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why does NumPy perform so much faster than normal Python arrays (Hint, isn't the lack of commas weird)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What about tensors? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Vectors are 1-D arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Matrices are 2-D arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Tensors are n-dimensional arrays of various sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to create a Torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "a = [[j+3*i for j in range(3)] for i in range(2)]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np_array = np.array(a)\n",
    "print(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Initialize from normal python array\n",
    "\n",
    "import torch\n",
    "a_data = torch.tensor(a)\n",
    "print(a_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Initialize from NumPy array\n",
    "\n",
    "\n",
    "a_np = torch.from_numpy(np_array)\n",
    "print(a_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Useful tensor initializations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "shape = (2,3,4)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape, dtype=int)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(\"Random Tensor: \\n {} \\n\".format(rand_tensor))\n",
    "print(\"Ones Tensor: \\n {} \\n\".format(ones_tensor))\n",
    "print(\"Zeros Tensor: \\n {}\".format(zeros_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some simple Tensor Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(\"Shape of tensor: {}\".format(tensor.shape))\n",
    "print(\"Datatype of tensor: {}\".format(tensor.dtype))\n",
    "print(\"Device tensor is stored on: {}\".format(tensor.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tensor indexing\n",
    "\n",
    "Tensors can be indexed just like NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "a = [[j+3*i for j in range(3)] for i in range(2)]\n",
    "a_tensor = torch.tensor(a)\n",
    "print(a_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# example of slicing\n",
    "print(a_tensor[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(a_tensor[1,0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Generate random integers\n",
    "aa = torch.randint(0, 100, (2, 3, 4))\n",
    "print(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(aa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Alertness Check - **What is the index of the value \"78\" ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(aa.shape)\n",
    "print(aa[0][2][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(aa.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Basic Tensor Operations\n",
    "\n",
    "Same idea as with NumPy arrays/matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "a = [[i+3*j for i in range(3)] for j in range(2)]\n",
    "b = [[j+2*i for i in range(3)] for j in range(2)]\n",
    "c = [[int(i+3*j%2==1) for i in range(2)] for j in range(3)]\n",
    "\n",
    "a_tensor = torch.tensor(a)\n",
    "b_tensor = torch.tensor(b)\n",
    "c_tensor = torch.tensor(c)\n",
    "print(a_tensor)\n",
    "print(b_tensor)\n",
    "print(c_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# This computes the ****** product\n",
    "print(f\"a_tensor.mul(b_tensor) \\n {a_tensor.mul(b_tensor)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"a_tensor * b_tensor \\n {a_tensor * b_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# This transposes a tensor\n",
    "print('Original tensor \\n {} \\n'.format(a_tensor))\n",
    "print('Transposed tensor \\n {} \\n'.format(a_tensor.T))\n",
    "print('Another way to transpose the tensor \\n {} \\n'.format(torch.transpose(a_tensor, dim0=0, dim1=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Alertness check: **Why is there a a dim0/dim1 option in tensor.transpose?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([[[i for k in range(4)] for j in range(3)] for i in range(2)])\n",
    "print(x.shape)\n",
    "print(\"Original Vector:\")\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's say we want to change x to be a $4\\times 3 \\times 2$ tensor? How to we transpose it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Original Vector:\")\n",
    "print(x)\n",
    "print(print(x.shape))\n",
    "print(\"Transposed Vector (Dim = ?):\")\n",
    "xt=torch.transpose(x, dim0=2, dim1=0)\n",
    "print(xt)\n",
    "print(print(xt.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img align=\"left\" src=\"img/transpose_example_2.png\" width=\"600px\" style=\"padding:30px;border:thin solid white;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What if we do x.T?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(x.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[Why?](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Elementwise operations\n",
    "\n",
    "Many functions can be applied on a tensor element by element\n",
    "- torch.cos\n",
    "- torch.sin\n",
    "- torch.exp\n",
    "- torch.log\n",
    "\n",
    "These functions **return a new tensor** with the function being applied to each element in the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.ones([4,4])\n",
    "b = torch.log(a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In-place operations\n",
    "\n",
    "Operations that have a ```_``` suffix are in-place. For example: ```x.copy_(y)```, ```x.t_()```, ```x.exp_()```, will change ```x```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interaction with NumPy\n",
    "\n",
    "You'll likely need to convert between a NumPy array (usually when plotting data/results). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Convert from tensor to NumPy array\n",
    "\n",
    "tensor = torch.ones(4, 4)\n",
    "print(tensor)\n",
    "print(tensor.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Covert from NumPy array to torch:\n",
    "\n",
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Cool thing is that changes in NumPy array are reflected in the tensor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Both numpy array and tensor data point to the same contiguous block in memory. Very different than view, speaking of which: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensor Views"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Imagine we want to access part of a tensor (e.g., first row) or we want to permute rows and columns. Do we really need to allocate new memory to store this part of the tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "No:\n",
    "- a tensor can be a \"view\" of an existing tensor\n",
    "- a \"view\" of a tensor shares the data with the original tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tensor([[4*i+j for j in range(4)] for i in range(4)])\n",
    "b = a.view(2, 8)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1] = 100\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How much memory does ```a``` and ```b``` consume?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(\"a is sized: \" + str(sys.getsizeof(a)))\n",
    "print(\"b is sized: \" + str(sys.getsizeof(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Makes sense right? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "c = torch.tensor([[4*i+j for j in range(100)] for i in range(100)])\n",
    "print(\"c is sized: \" + str(sys.getsizeof(c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Wait what? Why is every sized the same?! c is so much bigger....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"The data in 'a' is sized: \" + str(a.element_size() * a.nelement()))\n",
    "print(\"The data in 'b' is sized: \" + str(b.element_size() * b.nelement()))\n",
    "print(\"The data in 'c' is sized: \" + str(c.element_size() * c.nelement()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.tensor([[4*i+j for j in range(4)] for i in range(4)])\n",
    "b = a.view(-1, 16)\n",
    "c = torch.tensor([[5*i+j for j in range(5)] for i in range(5)])\n",
    "\n",
    "def same_storage(x, y):\n",
    "    return x.storage().data_ptr() == y.storage().data_ptr()\n",
    "\n",
    "print(\"Do 'a' and 'b' share a view? \" + str(same_storage(a, b)))\n",
    "print(\"Do 'a' and 'c' share a view? \" + str(same_storage(a, c))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hard to know what creates a view and what creates a copy, but general rule of thumb is that a view is created unless there are no other options: \n",
    "\n",
    "|Creates tensor views| Creates tensor copies|\n",
    "|:-------------------|:------------------|\n",
    "|  ```view()``` - reshapes tensor     |   ```clone()``` - explicity creates a copy|\n",
    "|  ```narrow()``` - extracts contiguous subset of tensor     |   ```detach()``` - creates a new tesnor detached from computational graph|\n",
    "| ```transpose()``` - swaps two dimensions of a tensor | ```copy() ``` - copies data from one tensor to another|\n",
    "| ```permute()``` - generalizes transpose to many dimensions | ```to() ``` - copies data to gpu|\n",
    "| ```squeeze()``` - removes dimensions of size 1 | |\n",
    "| ```unsqueeze()``` - add dimension of size 1 | |\n",
    "| ```as_strided()``` - creates a view with non-contiguous strides (advanced) | |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tensor strides\n",
    "\n",
    "How do we create a new tensor with different dimensions, but with the same data? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "strides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Imagine that this is a tensor: \n",
    "\n",
    "<img align=\"left\" src=\"img/stride_tensor_example.png\" width=\"300px\" style=\"padding:30px;border:thin solid white;\"> \n",
    "\n",
    "The tensor class hold many properties: \n",
    "\n",
    "- sizes: (w,h,d)\n",
    "- dtype: int/float/...\n",
    "- device: cpu/gpu\n",
    "- layout: strided\n",
    "- strides: (w*h, w, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Strided representation: \n",
    "\n",
    "<img align=\"left\" src=\"img/stride_mapping_example.png\" width=\"450px\" style=\"padding:30px;border:thin solid white;\"> \n",
    "\n",
    "So for instance, the element at index [1,0] corresponds to the memory at: base_ptr + (1*stride[0] + 0*stride[1])*size(dtype)\n",
    "\n",
    "For the left example it would be: 0x10+(1*2+0*1)*4=0x18\n",
    "\n",
    "*credit to: http://blog.ezyang.com/2019/05/pytorch-internals/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Combining Tensors\n",
    "\n",
    "We can concatenate tensors along different dimensions **that already exist**! All other dimensions must match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.randn(2,3)\n",
    "b = torch.cat((a,a,a),dim=0)\n",
    "c = torch.cat((a,a,a),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('Original tensor')\n",
    "print(a.shape)\n",
    "print(a)\n",
    "print('')\n",
    "print('Concatenate in dimension 0 (rows)')\n",
    "print(b.shape)\n",
    "print(b)\n",
    "print('')\n",
    "print('Concatenate in dimension 1 (rows)')\n",
    "print(c.shape)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Alertness check: Do ```a``` and ```b``` share the same storage? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(same_storage(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also stack tensors along a **new dimension**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.randn(2,3)\n",
    "b = torch.stack((a,a,a,a),dim=0)\n",
    "c = torch.stack((a,a,a,a),dim=1)\n",
    "d = torch.stack((a,a,a,a),dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('Original tensor')\n",
    "print(a.shape)\n",
    "print(a)\n",
    "print('')\n",
    "print('Stack in dimension 0')\n",
    "print(b.shape)\n",
    "print(b)\n",
    "print('')\n",
    "print('Stack in dimension 1')\n",
    "print(c.shape)\n",
    "print(c)\n",
    "print('Stack in dimension 2')\n",
    "print(d.shape)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Adding dimensions\n",
    "\n",
    "We can also add dimensions if we need them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.randn(2,3)\n",
    "b = a.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(a.shape)\n",
    "print(b.shape)\n",
    "c = b.squeeze(0)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Converting from one datatype to another\n",
    "\n",
    "Use the ```Tensor.to(...)``` function but keep in mind possible loss of bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.ones((2,4), dtype=torch.int32)\n",
    "print(x)\n",
    "y = x.to(torch.float64)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Can do this to save space. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## That is it for now\n",
    "\n",
    "### Will discuss storage, indexing, data-types, and functions next time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2,3,4],[4,5,6,7],[7,8,9,10]])\n",
    "a = a.float()\n",
    "n = 2\n",
    "b =  a[n:a.shape[0]:n,n:a.shape[1]:n]\n",
    "print(b)\n",
    "print(a.data_ptr())\n",
    "print(b.data_ptr())  # same pointer (expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
