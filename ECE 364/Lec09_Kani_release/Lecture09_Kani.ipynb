{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img align=\"left\" src=\"img/ECE364-logo.png\" width=\"300px\" style=\"padding:30px;border:thin solid white;\"> \n",
    "\n",
    "# Lecture 9 - Classification and Logisitic regression\n",
    "## ECE364 - Programming Methods for Machine Learning\n",
    "### Nickvash Kani \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### Slides based off prior lectures by Alex Schwing, Aigou Han, Farzad Kamalabadi, Corey Snyder. All mistakes are my own!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Things we'll discuss in this lecture\n",
    "\n",
    "- Linear classification\n",
    "- Weaknesses of linear classification \n",
    "- Support vector machines\n",
    "- Logistic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification introduction \n",
    "\n",
    "In the previous lectures, we talked about linear regression, i.e. fitting a linear model to data so we can use that model in the future to predict a dependent variable for some input. \n",
    "\n",
    "But lots of data is not continuous. Think of gender, race, species, ... none of those things are continous variables you can fit data to. What we need are models that can predict classifications, 0/1's yes/no's, etc. \n",
    "\n",
    "Next two lectures are on classifiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Binary linear classification\n",
    "\n",
    "- **classification**: predict a discrete-valued target\n",
    "- **binary**: predict a binary target $t \\in \\{0,1\\}$\n",
    "  - Training examples with $t = 1$ are called **positive examples**, and training examples with $t = 0$ are called **negative examples**.\n",
    "- **linear**: model is a linear function of **x**, followed by a threshold:\n",
    "\n",
    "$$\n",
    "z = \\mathbf{w}^T \\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "y =\n",
    "\\begin{cases}\n",
    "  1 & \\text{if } z \\geq r \\\\\n",
    "  0 & \\text{if } z < r\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some simplifications\n",
    "\n",
    "**Eliminating the threshold**\n",
    "\n",
    "- We can assume **WLOG** (Without Loss Of Generality) that the threshold $r = 0$:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^T \\mathbf{x} + b \\geq r \\Longleftrightarrow \\mathbf{w}^T \\mathbf{x} + b - r \\geq 0.\n",
    "$$\n",
    "\n",
    "Define $b' = b - r$:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^T \\mathbf{x} + b' \\geq 0.\n",
    "$$\n",
    "\n",
    "**Eliminating the bias**\n",
    "\n",
    "- Add a dummy feature $x_0$ which always takes the value 1. The weight $w_0$ is equivalent to a bias.\n",
    "\n",
    "**Simplified model**\n",
    "\n",
    "$$\n",
    "z = \\mathbf{w}^T \\mathbf{x}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y =\n",
    "\\begin{cases}\n",
    "  1 & \\text{if } z \\geq 0 \\\\\n",
    "  0 & \\text{if } z < 0\n",
    "\\end{cases}\n",
    "=H(z)\n",
    "$$\n",
    "\n",
    "Where $H(x)$ is the Heaviside step function. It's very similar to the $\\text{sign}(x)$ which steps between -1 and 1. \n",
    "\n",
    "[1] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some simple binary functions using classifiers [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " ### NOT\n",
    "\n",
    "<div style=\"width: 150px;\">\n",
    "\n",
    "| $x_0$ | $x_1$ | | $t$ |\n",
    "|:------|:------|:-:|:----|\n",
    "|   1   |   0   | |  1  |\n",
    "|   1   |   1   | |  0  |\n",
    "\n",
    "</div>\n",
    "\n",
    "$$\n",
    "b > 0 \\\\\n",
    "b + w < 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = 1, \\quad w = -2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### AND\n",
    "<div style=\"width: 200px;\">\n",
    "\n",
    "| $x_0$ | $x_1$ | $x_2$ | | $t$ |\n",
    "|:------|:------|:------|:-:|:----|\n",
    "|   1   |   0   |   0   | |  0  |\n",
    "|   1   |   0   |   1   | |  0  |\n",
    "|   1   |   1   |   0   | |  0  |\n",
    "|   1   |   1   |   1   | |  1  |\n",
    "\n",
    "</div>    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "b &< 0 \\\\\n",
    "b + w_2 &< 0 \\\\\n",
    "b + w_1 &< 0 \\\\\n",
    "b + w_1 + w_2 &> 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = -1.5, \\quad w_1 = 1, \\quad w_2 = 1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### XOR\n",
    "<div style=\"width: 200px;\">\n",
    "\n",
    "| $x_0$ | $x_1$ | $x_2$ | | $t$ |\n",
    "|:------|:------|:------|:-:|:----|\n",
    "|   1   |   0   |   0   | |  0  |\n",
    "|   1   |   0   |   1   | |  1  |\n",
    "|   1   |   1   |   0   | |  1  |\n",
    "|   1   |   1   |   1   | |  0  |\n",
    "\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "XOR is not linearly seperable. \n",
    "\n",
    "<img align=\"center\" src=\"img/linearly_seperable_xor.png\" width=\"300px\" style=\"padding:30px;border:thin solid white;\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Quick aside, XOR in particular can be linearly seperable if we choose a good feature map. But not all functions can be linearly seperated this way: \n",
    "\n",
    "$$\n",
    "\\phi(\\mathbf{x}) = \n",
    "\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_1 x_2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "<div style=\"width: 300px;\">\n",
    "\n",
    "| $x_1$ | $x_2$ | $\\phi_1 $ | $\\phi_2$ | $\\phi_3$ | | $t$ |\n",
    "|:-----|:-----|:---------------------|:---------------------|:---------------------|:-:|:---|\n",
    "|   0   |   0   |           0           |           0           |           0           | |  0  |\n",
    "|   0   |   1   |           0           |           1           |           0           | |  1  |\n",
    "|   1   |   0   |           1           |           0           |           0           | |  1  |\n",
    "|   1   |   1   |           1           |           1           |           1           | |  0  |\n",
    "\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Too much data for classification? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose we have the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "x1 = torch.cat([0.5*torch.randn((100,))-2, 0.5*torch.randn((100,))+2],dim=0)\n",
    "y = torch.cat([-torch.ones(100,1), torch.ones(100,1)],dim=0)\n",
    "plt.plot(x1[0:100],y[0:100],'rx')\n",
    "plt.plot(x1[100:200],y[100:200],'bo')\n",
    "plt.xlabel('x_0')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Want to do a linear classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So we do the same steps as normal: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ w = (XX^T)^{-1}Xy$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "XT = torch.cat([x1.unsqueeze(0),torch.ones_like(x1).unsqueeze(0)],dim=0).t()\n",
    "w = torch.inverse(XT.t()@XT)@XT.t()@y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xaxis = torch.linspace(-3,3,100)\n",
    "regressionoutput = torch.cat([xaxis.unsqueeze(0),torch.ones_like(xaxis).unsqueeze(0)],dim=0).t()@w\n",
    "clfoutput = torch.sign(regressionoutput)\n",
    "plt.plot(x1[0:100],y[0:100],'rx')\n",
    "plt.plot(x1[100:200],y[100:200],'bo')\n",
    "plt.plot(xaxis,regressionoutput,'-mv')\n",
    "plt.plot(xaxis,clfoutput,'-k')\n",
    "plt.xlabel('x_1')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's say we get even more data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x1_new = torch.cat([x1, 0.5*torch.randn((100,))+10],dim=0)\n",
    "y_new = torch.cat([y, torch.ones(100,1)],dim=0)\n",
    "xaxis = torch.linspace(-3,11,100)\n",
    "regressionoutput_old = torch.cat([xaxis.unsqueeze(0),torch.ones_like(xaxis).unsqueeze(0)],dim=0).t()@w\n",
    "clfoutput_old = torch.sign(regressionoutput_old)\n",
    "plt.plot(x1_new[0:100],y_new[0:100],'rx')\n",
    "plt.plot(x1_new[100:],y_new[100:],'bo')\n",
    "plt.plot(xaxis,regressionoutput_old,'-mv')\n",
    "plt.plot(xaxis,clfoutput_old,'-k')\n",
    "plt.xlabel('x_1')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Looks good. Old model still works visually but just to be sure, let's do the fit again...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "XT_new = torch.cat([x1_new.unsqueeze(0),torch.ones_like(x1_new).unsqueeze(0)],dim=0).t()\n",
    "w_new = torch.inverse(XT_new.t()@XT_new)@XT_new.t()@y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "regressionoutput_new = torch.cat([xaxis.unsqueeze(0),torch.ones_like(xaxis).unsqueeze(0)],dim=0).t()@w_new\n",
    "clfoutput_new = torch.sign(regressionoutput_new)\n",
    "plt.plot(x1_new[0:100],y_new[0:100],'rx')\n",
    "plt.plot(x1_new[100:],y_new[100:],'bo')\n",
    "plt.plot(xaxis,regressionoutput_old,'-mv')\n",
    "plt.plot(xaxis,clfoutput_old,'-k')\n",
    "plt.plot(xaxis,regressionoutput_new,'-cv')\n",
    "plt.plot(xaxis,clfoutput_new,'-y')\n",
    "plt.xlabel('x_1')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How come this new easy data screws up things?\n",
    "- Shouldn't we get a better classifier if we train with more data?\n",
    "- What is going wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The big issue with the previous graph was that all points contributed equally to the boundary decision. \n",
    "\n",
    "But should we really do this? Isn't it more important to just make sure everything is on the correct side of the barrier? and move on? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's consider a new motivating example: \n",
    "\n",
    "<img align=\"center\" src=\"img/SVM_example_2D.png\" width=\"600px\" style=\"padding:30px;border:thin solid white;\"> \n",
    "\n",
    "Thanks to [2] for a great image and explanation of SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's assume blue (+1) circles are in the positive class and red x's (-1) are in the negative class (we'll call this the target ($t$)). For each training point $x^{(i)}$, we want: \n",
    "\n",
    "- $W x^{(i)} > 1$ if $t^{(i)} = 1$\n",
    "- $W x^{(i)} < -1$ if $t^{(i)} = -1$\n",
    "\n",
    "if we use the label $t^{(i)}$, we can get the following for both cases: $t^{(i)} \\cdot W x^{(i)} > 1$ or  $1 - t^{(i)} \\cdot W x^{(i)} < 0$.\n",
    "\n",
    "This inequality is satisfied as long as $\\vert Wx^(i) \\vert > 1$ but beyond that point, why should we care? Why not just tell the model to work to satisfy this inequality but no more. We do this by using the max function $\\max \\left( 0,1 - t^{(i)} \\cdot W x^{(i)} \\right)$ so the total loss function becomes: \n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\sum \\max\\left( 0, 1- t^{(i)} \\cdot W x^{(i)} \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can figure out the width of the margin. The top line has the formula $Wx_1=1$ and the bottom has the formula $Wx_2=-1$. Subtract both sides of both equations gives us: $W * (x_1-x_2)[1:] = 2 $ which means $ (x_1-x_2)[1:] = \\frac{2}{W}$ (getting rid of the bias term). Hence the distance becomes $ \\vert x \\vert  = \\frac{2}{\\vert W \\vert }$.\n",
    "\n",
    "We want to maximize the margin $\\frac{2}{\\vert W \\vert }$ which is the same as minimizing the value $\\frac{\\vert W[1:] \\vert}{2}$. Hence the loss function becomes: \n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{\\vert W[1:] \\vert}{2} + C\\sum \\max\\left( 0, 1 - t^{(i)} \\cdot W x^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "We included a term $C$ to tell the modify how much to priortize the margin vs fit of the boundary. This is the loss function we need to minimize.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So let's test our linear SVM model in code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "xa, ta = make_classification(n_features=2, n_redundant=0, random_state=25, n_informative=2, n_clusters_per_class=2, n_classes=2)\n",
    "for i in range(len(ta)):\n",
    "    if ta[i] == 0: ta[i] = 1\n",
    "    else: ta[i] = -1\n",
    "x = torch.tensor(xa)\n",
    "t = torch.tensor(ta, dtype=torch.float64)\n",
    "X = torch.column_stack((torch.ones(x.shape[0]), x))\n",
    "\n",
    "# initialize guesses for w, b\n",
    "w_gd = torch.randn((3), requires_grad=True, dtype=torch.float64) # size (1,)\n",
    "print('Initial guesses: w0={:.6f}, w1={:.6f}, w2={:.6f}'.format(w_gd[0].data, w_gd[1].data, w_gd[2].data))\n",
    "                       \n",
    "                       \n",
    "# information for tracking\n",
    "b_vals = [w_gd[0].data.item()]\n",
    "w1_vals = [w_gd[1].data.item()]\n",
    "w2_vals = [w_gd[2].data.item()]\n",
    "\n",
    "C = 500                     \n",
    "# gradient descent loop\n",
    "n_iter = 1000 # number of iterations\n",
    "alpha = 1e-6 # step size\n",
    "for n in range(n_iter):\n",
    "    temp = t*(X@w_gd)\n",
    "    error = torch.ones(X.shape[0])-t*(X@w_gd)\n",
    "    loss_lin, q = torch.max(torch.column_stack((torch.zeros(X.shape[0]), error)), dim=1)\n",
    "    #loss = loss_lin.t()@loss_lin\n",
    "    loss = w_gd.t()@w_gd + C*(loss_lin.t()@loss_lin)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w_gd -= alpha*w_gd.grad\n",
    "        w_gd.grad = None\n",
    "        \n",
    "    # log information\n",
    "    w2_vals.append(w_gd[2].data.item())\n",
    "    w1_vals.append(w_gd[1].data.item())\n",
    "    b_vals.append(w_gd[0].data.item())\n",
    "    \n",
    "# examine solution\n",
    "print('Final guesses: w0={:.6f}, w1={:.6f}, w2={:.6f}'.format(w_gd[0].data, w_gd[1].data, w_gd[2].data))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(xa[t == -1, 0], xa[t == -1, 1], c='blue', marker='o', label='Class -1')\n",
    "ax.scatter(xa[t == 1, 0], xa[t == 1, 1], c='red',  marker='x', label='Class 1')\n",
    "linestyles = ['dashed', 'solid', 'dashed']\n",
    "linecolors = ['blue', 'green', 'red']\n",
    "offsets = [1, 0, -1]\n",
    "for i in range(len(offsets)):\n",
    "    xx = np.arange(min(x[:,0]),max(x[:,0]),0.1)\n",
    "    yy = -1*(offsets[i]+w1_vals[-1]*xx + b_vals[-1])/w2_vals[-1]\n",
    "    ax.plot(xx,yy,linestyle=linestyles[i], color=linecolors[i])\n",
    "plt.axis([min(x[:,0]),max(x[:,0]), min(x[:,1]),max(x[:,1])])\n",
    "# Save and Show\n",
    "# plt.savefig(\"./img/losses_contours.png\", dpi=300)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's refer back to our motivating graph and use support vector machines to classify it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "xa = torch.cat([0.5*torch.randn((100,))-2, 0.5*torch.randn((100,))+2],dim=0)\n",
    "ta = torch.cat([-torch.ones(100,1), torch.ones(100,1)],dim=0)\n",
    "x = torch.tensor(xa, dtype=torch.float64)\n",
    "t = torch.tensor(ta, dtype=torch.float64)\n",
    "X = torch.column_stack((torch.ones(x.shape[0]), x))\n",
    "\n",
    "# initialize guesses for w, b\n",
    "w_gd = torch.randn((2), requires_grad=True, dtype=torch.float64) # size (1,)\n",
    "print('Initial guesses: w0={:.6f}, w1={:.6f}'.format(w_gd[0].data, w_gd[1].data))\n",
    "                         \n",
    "# information for tracking\n",
    "b_vals = [w_gd[0].data.item()]\n",
    "w1_vals = [w_gd[1].data.item()]\n",
    "\n",
    "C = 500                     \n",
    "# gradient descent loop\n",
    "n_iter = 1000 # number of iterations\n",
    "alpha = 1e-6 # step size\n",
    "for n in range(n_iter):\n",
    "    temp = t*(X@w_gd)\n",
    "    error = torch.ones(X.shape[0])-t*(X@w_gd)\n",
    "    loss_lin, q = torch.max(torch.column_stack((torch.zeros(X.shape[0]), error)), dim=1)\n",
    "    #loss = loss_lin.t()@loss_lin\n",
    "    loss = w_gd.t()@w_gd + C*(loss_lin.t()@loss_lin)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w_gd -= alpha*w_gd.grad\n",
    "        w_gd.grad = None\n",
    "        \n",
    "    # log information\n",
    "    w1_vals.append(w_gd[1].data.item())\n",
    "    b_vals.append(w_gd[0].data.item())\n",
    "    \n",
    "# examine solution\n",
    "print('Final guesses: w0={:.6f}, w1={:.6f}'.format(w_gd[0].data, w_gd[1].data))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plt.plot(xa[0:100],ta[0:100],'rx')\n",
    "plt.plot(xa[100:200],ta[100:200],'bo')\n",
    "linestyles = ['dashed', 'solid', 'dashed']\n",
    "linecolors = ['blue', 'green', 'red']\n",
    "offsets = [-1, 0, 1]\n",
    "for i in range(len(offsets)):\n",
    "    xx = np.arange(min(x),max(x),0.1)\n",
    "    yy = offsets[i]+w1_vals[-1]*xx + b_vals[-1]\n",
    "    ax.plot(xx,yy,linestyle=linestyles[i], color=linecolors[i])\n",
    "plt.axis([min(xa),max(xa), -1.2, 1.2])\n",
    "# Save and Show\n",
    "# plt.savefig(\"./img/losses_contours.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Instead of regressing a line $w^Tx$ let's regress the sigmoid function $$\\frac{1}{1+\\exp(-w^Tx)} \\in [0,1]$$\n",
    "\n",
    "How, i.e., what's the goal?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- whenever an annotated datapoint $(x^{(i)}, t^{(i)})$ has a label $t^{(i)} = 1$ we want the sigmoid function to be as large as possible, i.e., as close as possible to 1\n",
    "- whenever an annotated datapoint $(x^{(i)}, t^{(i)})$ has a label $t^{(i)} = -1$ we want the sigmoid function to be as small as possible, i.e., as close as possible to 0\n",
    "- this last point can be formulated differently: whenever an annotated datapoint $(x^{(i)}, t^{(i)})$ has a label $t^{(i)} = -1$ we want one minus the sigmoid function to be as large as possible, i.e., as close as possible to 1: $$1 - \\frac{1}{1+\\exp(-w^Tx)} = \\frac{1}{1+\\exp(w^Tx)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Taking into account $t^{(i)}\\in\\{-1,1\\}$ we can combine both goals (same logic as before!):\n",
    "- our goal is to make the following as large as possible: $$\\frac{1}{1+\\exp(-t^{(i)}w^Tx^{(i)})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Because this function varies between 0 and 1, it is useful to look at this as a probability that the input is classified as one category or another: $$p(Y=t|x) = \\frac{1}{1+\\exp(-tw^Tx)}$$\n",
    "\n",
    "For notational convenience we instead often write $$p(t|x) = \\frac{1}{1+\\exp(-tw^Tx)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next we got to combine the losses over multiple dataset samples: ${\\cal D} = \\{(x^{(i)}, t^{(i)})\\}$?\n",
    "\n",
    "Since we are dealing with probabilities, summing makes little sense. Instead we will use multiplication: \n",
    "\n",
    "$$p(t^{(1)}, \\dots, t^{(|{\\cal D}|)}|x^{(1)}, \\dots, x^{(|{\\cal D}|)}) = \\prod_{(x^{(i)},t^{(i)})\\in{\\cal D}} p(t^{(i)}|x^{(i)})$$\n",
    "\n",
    "We wish to maximize the probability every prediction matches the data (generally referred to as maximum likelihood):\n",
    "\n",
    "$$\\arg\\max_w \\prod_{(x^{(i)},t^{(i)})\\in{\\cal D}} p(t^{(i)}|x^{(i)}) = $$\n",
    "\n",
    "Adding a monotonic transformation function like a log doesn't change the **maximizing argument**\n",
    "\n",
    "$$ \\arg\\max_w \\log\\prod_{(x^{(i)},t^{(i)})\\in{\\cal D}} p(t^{(i)}|x^{(i)}) = $$\n",
    "\n",
    "And since PyTorch is more focused on minimizing functions, we can reformulate this a bit: \n",
    "\n",
    "$$ \\arg\\min_w \\sum_{(x^{(i)},t^{(i)})\\in{\\cal D}} -\\log p(t^{(i)}|x^{(i)}) = $$\n",
    "\n",
    "This is why people also call this **minimizing the negative log-likelihood (=maximizing the likelihood)**\n",
    "\n",
    "$$ \\arg\\min_w \\sum_{(x^{(i)},t^{(i)})\\in{\\cal D}} \\log (1 + \\exp(-t^{(i)}w^Tx^{(i)}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we have a new problem formulation (a new program which differs from linear regression). But we still need to find the parameters $w$ that minimize this function. \n",
    "\n",
    "Time to take the derivative and follow the gradient to the minimum value of $w$: \n",
    "\n",
    "$$ \\frac{\\partial}{\\partial w} \\sum_{(x^{(i)},t^{(i)})\\in{\\cal D}} \\log (1 + \\exp(-t^{(i)}w^Tx^{(i)}))$$\n",
    "\n",
    "doing out the derivative: \n",
    "\n",
    "\n",
    "$$ \\sum_{(x^{(i)},t^{(i)})\\in{\\cal D}} \\frac{1}{1+\\exp(-t^{(i)}w^Tx^{(i)})}\\cdot \\exp(-t^{(i)}w^Tx^{(i)}) \\cdot (-t^{(i)}x^{(i)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(10)\n",
    "np.random.seed(10)\n",
    "\n",
    "xa = torch.cat([0.5*torch.randn((100,))-2, 0.5*torch.randn((100,))+2],dim=0)\n",
    "ta = torch.cat([-torch.ones(100,1), torch.ones(100,1)],dim=0)\n",
    "x = torch.tensor(xa, dtype=torch.float64)\n",
    "t = torch.tensor(ta, dtype=torch.float64)\n",
    "X = torch.column_stack((torch.ones(x.shape[0]), x))\n",
    "\n",
    "\n",
    "# initialize guesses for w, b\n",
    "w_gd = torch.randn((2), requires_grad=True, dtype=torch.float64) # size (1,)\n",
    "print('Initial guesses: w0={:.6f}, w1={:.6f}'.format(w_gd[0].data, w_gd[1].data))\n",
    "                         \n",
    "# information for tracking\n",
    "b_vals = [w_gd[0].data.item()]\n",
    "w1_vals = [w_gd[1].data.item()]\n",
    "\n",
    "n_iter = 1000 # number of iterations\n",
    "alpha = 1e-3 # step size\n",
    "for n in range(n_iter):\n",
    "    loss = torch.sum(torch.log(1+torch.exp(-(X@w_gd)*t)))\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w_gd -= alpha*w_gd.grad\n",
    "        w_gd.grad = None\n",
    "        \n",
    "    # log information\n",
    "    w1_vals.append(w_gd[1].data.item())\n",
    "    b_vals.append(w_gd[0].data.item())\n",
    "    \n",
    "# examine solution\n",
    "print('Final guesses: w0={:.6f}, w1={:.6f}'.format(w_gd[0].data, w_gd[1].data))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plt.plot(xa[0:100],ta[0:100],'rx')\n",
    "plt.plot(xa[100:200],ta[100:200],'bo')\n",
    "linestyles = ['dashed', 'solid', 'dashed']\n",
    "linecolors = ['blue', 'green', 'red']\n",
    "offsets = [-1, 0, 1]\n",
    "xx = torch.arange(min(x),max(x),0.01)\n",
    "yy = 1/(1+torch.exp(-(w1_vals[-1]*xx+b_vals[-1])))\n",
    "ax.plot(xx,yy)\n",
    "plt.axis([min(xa),max(xa), -1.2, 1.2])\n",
    "# Save and Show\n",
    "# plt.savefig(\"./img/losses_contours.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Quick aside\n",
    "\n",
    "If you play around with the random seed in th eabove example, you might see that the logistic regression appears \"flipped.\"\n",
    "\n",
    "This behavior is expected and comes from a **symmetry / non-identifiability** in our setup.\n",
    "\n",
    "We are minimizing\n",
    "\n",
    "$$\n",
    "\\sum_i \\log\\left(1+\\exp\\big(-t_i (w_0 + w_1 x_i)\\big)\\right),\n",
    "$$\n",
    "\n",
    "with labels $t_i\\in\\{-1,+1\\}$ and a model $z = w_0 + w_1 x$.\n",
    "\n",
    "Since $x$ and the labels are all symmetrical, the loss depends *only* on the margin $t_i z_i$. It does **not** directly constrain:\n",
    "\n",
    "- the sign of $w_1$\n",
    "- where the sigmoid is centered\n",
    "- which side of the plot corresponds to probability 1\n",
    "\n",
    "With symmetric data (two Gaussian blobs centered at ±2) and a free bias term, there are multiple parameter settings that give low loss:\n",
    "\n",
    "- an intuitive solution: positive slope, centered sigmoid\n",
    "- a “degenerate” solution: negative slope + large bias, pushing the sigmoid far to one side\n",
    "\n",
    "Both satisfy $t_i z_i > 0$ for most points, so both are valid minima of the objective.\n",
    "\n",
    "Gradient descent may converge to either depending on initialization.\n",
    "\n",
    "This is a classic example of **parameter non-identifiability** in unregularized logistic regression. There are ways to fix this but this is only an illustraive example that works well with the prior thoughts so I'm keeping it as it is.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## That's it for today\n",
    "\n",
    "- Have a good weekend\n",
    "- HW4 due Monday\n",
    "- Continue talking about classification and logistic regression Tuesday "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Additional References\n",
    "\n",
    "[1] Roger Grosse CSC321 lectures - https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/\n",
    "\n",
    "[2] Marton Trencseni \"SVM with PyTorch\" https://bytepawn.com/svm-with-pytorch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
