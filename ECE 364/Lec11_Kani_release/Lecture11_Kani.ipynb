{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <img align=\"left\" src=\"img/ECE364-logo.png\" width=\"300px\" style=\"padding:30px;border:thin solid white;\"> \n",
    "\n",
    "# Lecture 11 - Pytorch NN module and optimizers\n",
    "## ECE364 - Programming Methods for Machine Learning\n",
    "### Nickvash Kani \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### Slides based off prior lectures by Alex Schwing, Aigou Han, Farzad Kamalabadi, Corey Snyder. All mistakes are my own!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this lecture: \n",
    "\n",
    "- Methods and attributes for a PyTorch ``nn.Module`` object.\n",
    "- Parametric model example using PyTorch ``nn.Module``. \n",
    "- PyTorch optimizers for training ``nn.Module`` object.\n",
    "- Recipe for using a PyTorch optimizer in a basic machine learning training loop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To recap what we have been doing for the last 2 weeks. We have: \n",
    "\n",
    "- Dataset $\\mathcal{D}=\\{(x_i, t_i)\\}_{i=0}^{N-1}$\n",
    "- Model $f_\\theta(x)$ with learnable parameters $\\theta$\n",
    "- Loss function $\\ell(f_\\theta(x_i), t_i)$\n",
    "- Learning algorithm to update parameter values using loss function, e.g. gradient descent is called the optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example: \n",
    "    \n",
    "In binary logistic regression we have: \n",
    "    \n",
    "* Dataset $\\mathcal{D}=\\{(x_i, t_i)\\}_{i=1}^{N}$, where $x_i\\in\\mathbb{R}^3$ and $t_i\\in\\{0, 1\\}$.\n",
    "* Model $f_\\theta(x)$ where\n",
    "  $$\n",
    "  f_\\theta(x) = \\mathbf{Pr}\\{t_i=1|x_i\\}=\\frac{1}{1+e^{-(w^\\top x_i+b)}}\n",
    "  $$\n",
    "  and $\\theta=\\{w, b\\}\\in\\mathbb{R}$.\n",
    "* The loss function may be binary cross-entropy $\\ell_{\\textrm{bce}}(f_\\theta(x_i), y_i)$\n",
    "  $$\n",
    "  \\ell_{\\textrm{bce}}(f_\\theta(x_i), y_i)=-\\left[(1-y_i)\\log(1-f_\\theta(x_i))+y_i\\log(f_\\theta(x_i))\\right]\n",
    "  $$\n",
    "* By chain rule (i.e. backpropagation), we may obtain $\\frac{\\partial \\ell_{\\textrm{bce}}}{\\partial w}$ and $\\frac{\\partial \\ell_{\\textrm{bce}}}{\\partial b}$ and use gradient descent with step-size or **learning rate** $\\alpha$ to find a good solution for the parameters over the dataset.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Brief review of objects and classes\n",
    "\n",
    "* **Class:** A collection of attributes and functions (methods) providing an interface to distinct instances and applications of these common attributes and functions.\n",
    "* **Object:** An instance of a class that can perform the desired functionalities which are defined in the class. Separate objects of a class may apply the functionality of a class differently. For example, a \"Fruit\" class may have attributes that can helpfully distinguish between apple and banana objects which are instances of the \"Fruit\" class.\n",
    "* **self:** The ``self`` identifier represents the instance of the class. By using the **self** keyword we can access the **attributes** and **methods** of the class in python or assign attributes of the given object within the class definition. \n",
    "* **\\_\\_init\\_\\_ :** The ``__init__`` method is a reseved method in Python classes. It is known as the **constructor** in object oriented programming . This method called when an object is created from the class and it allow the class to initialize the attributes of a class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ``nn.Module`` class\n",
    "\n",
    "The [``nn.Module`` class](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) is the universal **base class** for neural networks, and more broadly trainable models, in PyTorch found within the ``torch.nn`` package, i.e. ``torch.nn.Module``. We create our own models by **inheriting** this base class and implementing the necessary methods for the class. There are two methods which must be implemented: ``__init__`` and ``forward``.\n",
    "\n",
    "The ``__init__`` method specifies the constructor and thus how every instance of this module must be initialized the ``__init__`` method must first call ``super().__init__()`` to call the constructor of the base ``nn.Module`` class (and thus access all the helpful attributes and methods within). The ``forward`` method implements how the model processes input data for its **forward pass**.\n",
    "\n",
    "As an example ``nn.Module`` class. Consider a class for implementing third-order polynomial regression. In this case, we have\n",
    "$$\n",
    "f_\\theta(x) = ax^3+bx^2+cx+d\n",
    "$$\n",
    "and $\\theta=\\{a, b, c, d\\}$. The corresponding ``nn.Module`` class may be written as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ThirdOrderPolynomial(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Specify the learnable parameters: a, b, c, d\n",
    "        '''\n",
    "        super().__init__() # call nn.Module constructor first\n",
    "        self.a = nn.Parameter(torch.rand(1))\n",
    "        self.b = nn.Parameter(torch.rand(1))\n",
    "        self.c = nn.Parameter(torch.rand(1))\n",
    "        self.d = nn.Parameter(torch.rand(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Implement f(x).\n",
    "        '''\n",
    "        f_x = self.a*x**3 + self.b*x**2 + self.c*x + self.d\n",
    "        return f_x\n",
    "\n",
    "# create ThirdOrderPolynomial object\n",
    "my_model = ThirdOrderPolynomial()\n",
    "# print model parameters\n",
    "print(my_model.a)\n",
    "print(my_model.b)\n",
    "print(my_model.c)\n",
    "print(my_model.d)\n",
    "x_input = torch.tensor([1])\n",
    "prediction = my_model(x_input)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Important things to note: \n",
    "\n",
    "- ``self`` parameter must be used to access class attributes and methods. \n",
    "    - When is ``self`` not needed? -> when variables of objects not needed\n",
    "- Model parameters may be specified by wrapping a tensor in the ``nn.Parameter`` class.\n",
    " - The ``nn.Module`` class automatically collects all learnable paramters in the ``.parameters()`` attribute. This makes scaling our models and learning algorithms dramatically easier!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In lecture notes: \n",
    "\n",
    "- ``self`` is not needed for static methods (normal functions defined inside a class for organizational purposes.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "learnable_parameters = my_model.parameters()\n",
    "print(learnable_parameters)\n",
    "for p in learnable_parameters:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Logistic Regression Module\n",
    "\n",
    "Implement a logistic regression model class for inputs $x_i\\in\\mathbb{R}^N$ and include a bias term. Recall that \n",
    "$$\n",
    "f_\\theta(x)=\\mathbf{Pr}\\{y=1|x\\}=\\frac{1}{1+e^{-(w^\\top x+b)}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, N):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.ones(N))\n",
    "        self.b = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 1/(1+torch.exp(-(self.w@x+self.b)))\n",
    "\n",
    "N = 1\n",
    "my_logreg = LogisticRegression(N)\n",
    "x = torch.randn(N)\n",
    "y = my_logreg(x)\n",
    "\n",
    "# Plot both on the same figure\n",
    "L = 100\n",
    "x_values = torch.linspace(-5, 5, L)\n",
    "with torch.no_grad():\n",
    "    y_values = [my_logreg(torch.tensor(x_values[i].view(1, -1))) for i in range(L)]\n",
    "    \n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(x_values.detach().numpy(), y_values, 'b', label=\"Logistic Function\")\n",
    "\n",
    "# Axis labels, legend, grid, etc.\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('$\\sigma$')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.ylim([0, 1.2])  # Adjust as needed\n",
    "plt.show()\n",
    "#plt.savefig(\"img/log_ce_loss_plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss functions\n",
    "\n",
    "Also within the ``torch.nn`` package is a variety of [loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions) implemented as classes. Thus, we may create instances of these loss functions with varying attributes that may be called while training models. Most of the functions are simply wrapper functions for functions within ``torch.nn.functional``; however, they provide a helpful interface to simplify training code.\n",
    "\n",
    "For example, the ``nn.MSELoss`` class implements mean squared error loss with an attribute to specify the \"reduction\" of the loss. This reduction parameter allows us to specify if we want the squared errors to be summed, averaged, or left as a tensor of squared errors. In the latter case, we may use this to only incorporate select squared errors, i.e. masking or re-weighting, for certain applications.\n",
    "\n",
    "In general, for model object named ``model``, inputs named ``inputs``, ground-truths named ``targets``, and loss function object named ``criterion``, we may succinctly compute the current loss for our model as follows.\n",
    "\n",
    "``predictions = model(inputs)``\n",
    "\n",
    "``loss = criterion(predictions, targets)``\n",
    "\n",
    "The result of such a loss function can then act as the seed node from which we backpropagate for model training. Similarly, we may implement a custom loss function ourself (as an nn.Module class or as a regular function). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion_mean = nn.MSELoss(reduction='mean')\n",
    "criterion_sum = nn.MSELoss(reduction='sum')\n",
    "criterion_none = nn.MSELoss(reduction='none')\n",
    "\n",
    "pred = torch.zeros(4)\n",
    "targets = torch.ones(4)\n",
    "\n",
    "print(criterion_mean(pred, targets))\n",
    "print(criterion_sum(pred, targets))\n",
    "print(criterion_none(pred, targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ``torch.optim`` package\n",
    "\n",
    "We now know how to create a PyTorch model and how to apply a loss function to a dataset and model outputs. We have previously seen how to use the ``.backward()`` method to initiate backpropagation. However, we found it quite tedious to update each individual parameter by gradient descent and to clear those gradients each iteration.\n",
    "\n",
    "The [``torch.optim`` package](https://pytorch.org/docs/stable/optim.html) contains many helpful optimizers, i.e. learning algorithms, and other useful interfaces to simplify the parameter updating process. The simplest optimizer in the ``torch.optim`` package is the ``optim.SGD``, which implements **stochastic gradient descent** (SGD). Also known as mini-batch gradient descent, SGD implements gradient descent except only computes the gradient over a random subset, or mini-batch, of the data. Thus, the computed gradient depends on a stochastic sample of the dataset. If the **batch size** for SGD is the entire dataset, then SGD simply becomes ordinary gradient descent.\n",
    "\n",
    "**Pros:**\n",
    "* Faster than gradient descent\n",
    "* Noisier gradients from random subsets can help exit local minima\n",
    "\n",
    "**Cons:**\n",
    "* Can have slower convergence due to noisy gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Momentum and Weight Decay\n",
    "\n",
    "The [``optim.SGD`` optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) implements other helpful features aside from basic gradient descent including **momentum** and **weight decay**. As mentioned above, the gradients from SGD may be noisy and susceptible to random outliers. Momentum is one way to smooth out gradient descent updates by incorporating a moving average of previous gradients. Let $\\mu\\in[0, 1)$ be the momentum parameter, $g^{(k)}$ be the gradient over the batch in iteration $k$, and $\\theta^{(k)}$ be the model parameters as iteration $k$. The learning algorithm for SGD with momentum and learning rate $\\alpha$ is then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "G^{(k)} &= g^{(k)} + \\mu G^{(k-1)}\\\\\n",
    "\\theta^{(k+1)} &= \\theta^{(k)} - \\alpha G^{(k)}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus, the gradients become as exponential moving average of previous gradients where all previous gradients contribute a weight of $\\mu^{k-m}$ from step $m$. Popular choices of $\\mu$ are larger values such as $0.9$ and $0.99$ so that the exponential moving average does not vanish too quickly.\n",
    "\n",
    "The weight decay parameter implements the same $L_2$ regularization we saw with linear regression. For loss function $\\ell(f_\\theta(x), y)$ and weight decay parameter $\\lambda$, the effective total loss function used for backpropagation is given by\n",
    "\n",
    "$$\n",
    "\\ell_{\\textrm{total}} = \\ell(f_\\theta(x), y)+\\frac{\\lambda}{2}\\lVert\\theta\\rVert_2^2.\n",
    "$$\n",
    "Thus, an additional component of $\\lambda\\theta$ is added to the gradient. The purpose of weight decay is to encourage model parameters to not be too large and thus prone to overfitting and dangerous outlier behavior. Common choices of weight decay vary by model type and size, however, a small value like $10^{-5}$ is often a safe starting point. Other parameters exist within the ``optim.SGD`` class, but these two are the most important to explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The ``torch.optim`` package contains [many other popular optimizers](https://pytorch.org/docs/stable/optim.html#algorithms), such as Adagrad and Adam, which are popular **adaptive gradient** methods where per-layer learning rates are automatically tuned during training. For now, we will focus on applying SGD.\n",
    "\n",
    "To initialize the SGD optimizer, and most other optimizers, we need to give the optimizer:\n",
    "* A generator instance for the model parameters, i.e. call ``model.parameters()`` from our ``nn.Module`` object instance.\n",
    "* Learning rate\n",
    "* Momentum parameter (optional)\n",
    "* Weight decay (optional)\n",
    "* and other optional parameters for fancier learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "optimizer = torch.optim.SGD(my_logreg.parameters(), lr=1e-3, momentum=0.99, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning Rate Adjustments\n",
    "\n",
    "- Lastly, the overall learning rate for a learning algorithm is commonly adjusted in practice. \n",
    " \n",
    "- The most common method for this is to simply lower the learning rate by some factor whenever the loss function or validation performance appears to plateau. \n",
    "\n",
    "- The intuition for this is that a smaller learning rate allows us to more carefully descend into the current local minimum the model is converging to and hopefully provide additional modest improvements in performance. \n",
    "\n",
    "- The ``torch.optim`` package provides helpful learning rate **scheduler** [interfaces](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) where different learning rate policies may be implemented, e.g. after some number of gradient descent updates, after loss plateaus, or different policies for variably increasing or decreasing the learning rate. We will not worry about these for now, but they are important to point out within this package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A PyTorch training loop\n",
    "\n",
    "Finally, we have all the necessary ingredients to create a PyTorch training loop! This training loop, while basic, forms the core of any model training code within PyTorch. Before executing the training loop, we need to ensure a few things are set:\n",
    "* The model is instantiated/initialized.\n",
    "* The dataset is prepared.\n",
    "* The loss function and optimizer are instantiated.\n",
    "\n",
    "With these in hand, the training loop takes on the following basic cycle for optimizer named ``optimizer`` and loss function named ``criterion``:\n",
    "1. Zero out the gradients using the optimizer using ``optimizer.zero_grad()``.\n",
    "2. Pass the current batch or entire dataset to the model to generate predictions.\n",
    "3. Calculate loss from ``criterion``.\n",
    "4. Backpropagate from loss value and perform gradient descent update by ``optimizer.step()``\n",
    "5. (Optional) Perform any desired logging, e.g. loss values, performance metrics, etc.\n",
    "\n",
    "And that's it! Let's get some practice with a toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plotting utility function to visualize model proabilities of positive class\n",
    "def plot_model_probs(model, plus_class, negative_class):\n",
    "    x = torch.linspace(-4, 4, 100)\n",
    "    y = torch.linspace(-4, 4, 100)\n",
    "    X, Y = torch.meshgrid(x, y, indexing='ij')\n",
    "    meshgrid_inputs = torch.stack((X.flatten(), Y.flatten()), dim=1).unsqueeze(-1)\n",
    "    with torch.no_grad():\n",
    "        meshgrid_outputs = logreg_model(meshgrid_inputs)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(X.numpy(), Y.numpy(), meshgrid_outputs.reshape(100, 100).numpy(), cmap='RdBu_r', levels=100)\n",
    "    plt.colorbar()\n",
    "    plt.title('Probability of positive class')\n",
    "    plt.scatter(plus_class[:, 0].numpy(), plus_class[:, 1].numpy(), color='tomato', s=50, edgecolor='black')\n",
    "    plt.scatter(negative_class[:, 0].numpy(), negative_class[:, 1].numpy(), color='cornflowerblue', s=50, edgecolor='black')\n",
    "    plt.tight_layout()\n",
    "\n",
    "# compute classification accuracy\n",
    "def model_accuracy(model, input_data, labels):\n",
    "    predictions = model(input_data.unsqueeze(-1)).squeeze(-1)\n",
    "    positive_preds = predictions >= 0.5\n",
    "    negative_preds = predictions < 0.5\n",
    "    n_correct = torch.sum(positive_preds*labels)+torch.sum(negative_preds*(1-labels))\n",
    "    return n_correct/len(labels)\n",
    "\n",
    "# prepare dataset\n",
    "N = 50 # 50 points per class\n",
    "plus_class = 0.75*torch.randn(N, 2) + torch.tensor([-1, 1])\n",
    "negative_class = 0.75*torch.randn(N, 2) + torch.tensor([1, -1])\n",
    "input_data = torch.cat((plus_class, negative_class), dim=0)\n",
    "labels = torch.cat((torch.ones(N), torch.zeros(N)))\n",
    "print(input_data.shape, labels.shape)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(plus_class[:, 0].numpy(), plus_class[:, 1].numpy(), color='tomato', s=50, edgecolor='black')\n",
    "plt.scatter(negative_class[:, 0].numpy(), negative_class[:, 1].numpy(), color='cornflowerblue', s=50, edgecolor='black')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# setup before training loop\n",
    "# set up model\n",
    "logreg_model = LogisticRegression(2)\n",
    "\n",
    "# loss function and optimizer\n",
    "criterion = nn.BCELoss(reduction='mean') # binary cross-entropy loss, use mean loss\n",
    "lr = 1e-2 # learning rate\n",
    "optimizer = torch.optim.SGD(logreg_model.parameters(), lr=lr) \n",
    "\n",
    "# plotting utility, initial model performance (before learning!)\n",
    "plot_model_probs(logreg_model, plus_class, negative_class) # initial \n",
    "print('Model accuracy: {:.3f}'.format(model_accuracy(logreg_model, input_data, labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Varying training parameters I\n",
    "In the above example, we chose a learning rate of $\\alpha=10^{-2}$, used no momentum, used no weight decay, and trained the model for 200 iterations.\n",
    "\n",
    "Compare the results of training with and without momentum, keeping the learning rate and number of iterations fixed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Part a)\n",
    "# loss function and optimizer\n",
    "criterion = nn.BCELoss(reduction='mean') # binary cross-entropy loss, use mean loss\n",
    "lr = 1e-2 # learning rate\n",
    "\n",
    "# without momentum\n",
    "plot_probs = True\n",
    "logreg_model = LogisticRegression(2)\n",
    "optimizer = torch.optim.SGD(logreg_model.parameters(), lr=lr) \n",
    "# training loop\n",
    "n_iter = 200\n",
    "batch_size = 16\n",
    "loss_values, accuracies = [], []\n",
    "for n in range(n_iter):\n",
    "    # zero out gradients\n",
    "    optimizer.zero_grad()\n",
    "    # sample random batch and pass to model\n",
    "    batch_indices = np.random.choice(np.arange(len(labels)), size=batch_size)\n",
    "    input_batch = input_data[batch_indices].unsqueeze(-1) # make dimensions match for matrix multiplication\n",
    "    label_batch = labels[batch_indices]\n",
    "    predictions = logreg_model(input_batch).squeeze(-1) # make dimensions match for loss function\n",
    "    # calculate loss\n",
    "    loss = criterion(predictions, label_batch)\n",
    "    # backpropagate and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # logging\n",
    "    loss_values.append(loss.item())\n",
    "    accuracies.append(model_accuracy(logreg_model, input_data, labels))\n",
    "# plot model probabilities\n",
    "if plot_probs:\n",
    "    plot_model_probs(logreg_model, plus_class, negative_class)\n",
    "    plt.savefig(\"./img/classification_no_momentum.png\")\n",
    "\n",
    "# plot loss values\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.semilogy(loss_values)\n",
    "plt.grid(True)\n",
    "plt.title('Without momentum')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.subplot(122)\n",
    "plt.plot(accuracies)\n",
    "plt.grid(True)\n",
    "plt.title('Without momentum')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Classification accuracy')\n",
    "plt.savefig(\"./img/lossacc_no_momentum.png\")\n",
    "\n",
    "# with momentum\n",
    "plot_probs = True\n",
    "logreg_model = LogisticRegression(2) # re-initialize model\n",
    "optimizer = torch.optim.SGD(logreg_model.parameters(), lr=lr, momentum=0.99) \n",
    "# training loop\n",
    "n_iter = 200\n",
    "batch_size = 16\n",
    "loss_values, accuracies = [], []\n",
    "for n in range(n_iter):\n",
    "    # zero out gradients\n",
    "    optimizer.zero_grad()\n",
    "    # sample random batch and pass to model\n",
    "    batch_indices = np.random.choice(np.arange(len(labels)), size=batch_size)\n",
    "    input_batch = input_data[batch_indices].unsqueeze(-1) # make dimensions match for matrix multiplication\n",
    "    label_batch = labels[batch_indices]\n",
    "    predictions = logreg_model(input_batch).squeeze(-1) # make dimensions match for loss function\n",
    "    # calculate loss\n",
    "    loss = criterion(predictions, label_batch)\n",
    "    # backpropagate and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # logging\n",
    "    loss_values.append(loss.item())\n",
    "    accuracies.append(model_accuracy(logreg_model, input_data, labels))\n",
    "# plot model probabilities\n",
    "if plot_probs:\n",
    "    plot_model_probs(logreg_model, plus_class, negative_class)\n",
    "    plt.savefig(\"./img/classification_w_momentum.png\")\n",
    "# plot loss values\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.semilogy(loss_values)\n",
    "plt.grid(True)\n",
    "plt.title('With momentum')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.subplot(122)\n",
    "plt.plot(accuracies)\n",
    "plt.grid(True)\n",
    "plt.title('With momentum')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Classification accuracy')\n",
    "plt.savefig(\"./img/lossacc_w_momentum.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./img/classification_no_momentum.png\" alt=\"Image 1\" width=\"800\"></td>\n",
    "        <td><img src=\"./img/classification_w_momentum.png\" alt=\"Image 2\" width=\"800\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"./img/lossacc_no_momentum.png\" alt=\"Image 3\" width=\"800\"></td>\n",
    "        <td><img src=\"./img/lossacc_w_momentum.png\" alt=\"Image 4\" width=\"800\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Setting**:\n",
    "* $\\mu=0.99$\n",
    "\n",
    "**Observations**:\n",
    "* The loss values are lower and the accuracy improves faster to the best possible accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Varying training parameters II\n",
    "\n",
    "Now add weight decay and observe the changes to the training loss and accuracy.\n",
    "\n",
    "**Setting**:\n",
    "* $\\lambda= 10^{-2}$\n",
    "\n",
    "**Observations**:\n",
    "* The boundary is a bit more gradual than without weight decay while retaining the same accuracy with slightly higher loss values (less overfitted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Part b)\n",
    "# loss function and optimizer\n",
    "criterion = nn.BCELoss(reduction='mean') # binary cross-entropy loss, use mean loss\n",
    "lr = 1e-2 # learning rate\n",
    "\n",
    "# no weight decay\n",
    "plot_probs = True\n",
    "logreg_model = LogisticRegression(2)\n",
    "optimizer = torch.optim.SGD(logreg_model.parameters(), lr=lr, momentum=0.99) \n",
    "# training loop\n",
    "n_iter = 200\n",
    "batch_size = 16\n",
    "loss_values, accuracies = [], []\n",
    "for n in range(n_iter):\n",
    "    # zero out gradients\n",
    "    optimizer.zero_grad()\n",
    "    # sample random batch and pass to model\n",
    "    batch_indices = np.random.choice(np.arange(len(labels)), size=batch_size)\n",
    "    input_batch = input_data[batch_indices].unsqueeze(-1) # make dimensions match for matrix multiplication\n",
    "    label_batch = labels[batch_indices]\n",
    "    predictions = logreg_model(input_batch).squeeze(-1) # make dimensions match for loss function\n",
    "    # calculate loss\n",
    "    loss = criterion(predictions, label_batch)\n",
    "    # backpropagate and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # logging\n",
    "    loss_values.append(loss.item())\n",
    "    accuracies.append(model_accuracy(logreg_model, input_data, labels))\n",
    "# plot model probabilities\n",
    "if plot_probs:\n",
    "    plot_model_probs(logreg_model, plus_class, negative_class)\n",
    "    plt.savefig(\"./img/classification_no_weightdecay.png\")\n",
    "# plot loss values\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.semilogy(loss_values)\n",
    "plt.grid(True)\n",
    "plt.title('Without weight decay')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.subplot(122)\n",
    "plt.plot(accuracies)\n",
    "plt.grid(True)\n",
    "plt.title('Without weight decay')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Classification accuracy')\n",
    "plt.savefig(\"./img/lossacc_no_weightdecay.png\")\n",
    "\n",
    "# with weight decay\n",
    "plot_probs = True\n",
    "logreg_model = LogisticRegression(2) # re-initialize model\n",
    "optimizer = torch.optim.SGD(logreg_model.parameters(), lr=lr, momentum=0.99, weight_decay=1e-2) \n",
    "# training loop\n",
    "n_iter = 200\n",
    "batch_size = 16\n",
    "loss_values, accuracies = [], []\n",
    "for n in range(n_iter):\n",
    "    # zero out gradients\n",
    "    optimizer.zero_grad()\n",
    "    # sample random batch and pass to model\n",
    "    batch_indices = np.random.choice(np.arange(len(labels)), size=batch_size)\n",
    "    input_batch = input_data[batch_indices].unsqueeze(-1) # make dimensions match for matrix multiplication\n",
    "    label_batch = labels[batch_indices]\n",
    "    predictions = logreg_model(input_batch).squeeze(-1) # make dimensions match for loss function\n",
    "    # calculate loss\n",
    "    loss = criterion(predictions, label_batch)\n",
    "    # backpropagate and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # logging\n",
    "    loss_values.append(loss.item())\n",
    "    accuracies.append(model_accuracy(logreg_model, input_data, labels))\n",
    "# plot model probabilities\n",
    "if plot_probs:\n",
    "    plot_model_probs(logreg_model, plus_class, negative_class)\n",
    "    plt.savefig(\"./img/classification_w_weightdecay.png\")\n",
    "# plot loss values\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.semilogy(loss_values)\n",
    "plt.grid(True)\n",
    "plt.title('With weight decay')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.subplot(122)\n",
    "plt.plot(accuracies)\n",
    "plt.grid(True)\n",
    "plt.title('With weight decay')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Classification accuracy')\n",
    "plt.savefig(\"./img/lossacc_w_weightdecay.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./img/classification_no_weightdecay.png\" alt=\"Image 1\" width=\"800\"></td>\n",
    "        <td><img src=\"./img/classification_w_weightdecay.png\" alt=\"Image 2\" width=\"800\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"./img/lossacc_no_weightdecay.png\" alt=\"Image 3\" width=\"800\"></td>\n",
    "        <td><img src=\"./img/lossacc_w_weightdecay.png\" alt=\"Image 4\" width=\"800\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Setting** (right):\n",
    "* $\\lambda= 10^{-2}$\n",
    "\n",
    "**Observations**:\n",
    "* The boundary is a bit more gradual than without weight decay while retaining the same accuracy with slightly higher loss values (less overfitted)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Exercise: Varying training parameters\n",
    "In the above example, we chose a learning rate of $\\alpha=10^{-2}$, used no momentum, used no weight decay, and trained the model for 200 iterations.\n",
    "\n",
    "a) Compare the results of training with and without momentum, keeping the learning rate and number of iterations fixed.\n",
    "\n",
    "**Setting**:\n",
    "* $\\mu=0.99$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Part a)\n",
    "# loss function and optimizer\n",
    "criterion = nn.BCELoss(reduction='mean') # binary cross-entropy loss, use mean loss\n",
    "lr = 1e-2 # learning rate\n",
    "\n",
    "# without momentum\n",
    "plot_probs = True\n",
    "logreg_model = LogisticRegression(2)\n",
    "optimizer = torch.optim.SGD(logreg_model.parameters(), lr=lr) \n",
    "# training loop\n",
    "n_iter = 200\n",
    "batch_size = 16\n",
    "loss_values, accuracies = [], []\n",
    "for n in range(n_iter):\n",
    "    # zero out gradients\n",
    "    optimizer.zero_grad()\n",
    "    # sample random batch and pass to model\n",
    "    batch_indices = np.random.choice(np.arange(len(labels)), size=batch_size)\n",
    "    input_batch = input_data[batch_indices].unsqueeze(-1) # make dimensions match for matrix multiplication\n",
    "    label_batch = labels[batch_indices]\n",
    "    predictions = logreg_model(input_batch).squeeze(-1) # make dimensions match for loss function\n",
    "    # calculate loss\n",
    "    loss = criterion(predictions, label_batch)\n",
    "    # backpropagate and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # logging\n",
    "    loss_values.append(loss.item())\n",
    "    accuracies.append(model_accuracy(logreg_model, input_data, labels))\n",
    "# plot model probabilities\n",
    "if plot_probs:\n",
    "    plot_model_probs(logreg_model, plus_class, negative_class)\n",
    "    plt.savefig(\"./img/classification_no_momentum.png\")\n",
    "# plot loss values\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.semilogy(loss_values)\n",
    "plt.grid(True)\n",
    "plt.title('Without momentum')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.subplot(122)\n",
    "plt.plot(accuracies)\n",
    "plt.grid(True)\n",
    "plt.title('Without momentum')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Classification accuracy')\n",
    "plt.savefig(\"./img/lossacc_no_momentum.png\")\n",
    "\n",
    "\n",
    "# with momentum\n",
    "plot_probs = True\n",
    "logreg_model = LogisticRegression(2) # re-initialize model\n",
    "optimizer = torch.optim.SGD(logreg_model.parameters(), lr=lr, momentum=0.5) \n",
    "# training loop\n",
    "n_iter = 200\n",
    "batch_size = 16\n",
    "loss_values, accuracies = [], []\n",
    "for n in range(n_iter):\n",
    "    # zero out gradients\n",
    "    optimizer.zero_grad()\n",
    "    # sample random batch and pass to model\n",
    "    batch_indices = np.random.choice(np.arange(len(labels)), size=batch_size)\n",
    "    input_batch = input_data[batch_indices].unsqueeze(-1) # make dimensions match for matrix multiplication\n",
    "    label_batch = labels[batch_indices]\n",
    "    predictions = logreg_model(input_batch).squeeze(-1) # make dimensions match for loss function\n",
    "    # calculate loss\n",
    "    loss = criterion(predictions, label_batch)\n",
    "    # backpropagate and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # logging\n",
    "    loss_values.append(loss.item())\n",
    "    accuracies.append(model_accuracy(logreg_model, input_data, labels))\n",
    "# plot model probabilities\n",
    "if plot_probs:\n",
    "    plot_model_probs(logreg_model, plus_class, negative_class)\n",
    "    plt.savefig(\"./img/classification_w_momentum.png\")\n",
    "# plot loss values\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.semilogy(loss_values)\n",
    "plt.grid(True)\n",
    "plt.title('With momentum')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.subplot(122)\n",
    "plt.plot(accuracies)\n",
    "plt.grid(True)\n",
    "plt.title('With momentum')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Classification accuracy')\n",
    "plt.savefig(\"./img/lossacc_w_momentum.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./img/classification_no_momentum.png\" alt=\"Image 1\" width=\"800\"></td>\n",
    "        <td><img src=\"./img/classification_w_momentum.png\" alt=\"Image 2\" width=\"800\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"./img/lossacc_no_momentum.png\" alt=\"Image 3\" width=\"800\"></td>\n",
    "        <td><img src=\"./img/lossacc_w_momentum.png\" alt=\"Image 4\" width=\"800\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Observations**:\n",
    "* The loss values are lower and the accuracy improves faster to the best possible accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## That's it for today\n",
    "\n",
    "- Have a good weekend\n",
    "- HW5 due Monday\n",
    "- With talk about PyTorch Dataset management Tuesday"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
