{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img align=\"left\" src=\"img/ECE364-logo.png\" width=\"300px\" style=\"padding:30px;border:thin solid white;\"> \n",
    "\n",
    "# Lecture 7 - Linear Regression\n",
    "## ECE364 - Programming Methods for Machine Learning\n",
    "### Nickvash Kani \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### Slides based off prior lectures by Alex Schwing, Aigou Han, Farzad Kamalabadi, Corey Snyder. All mistakes are my own!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Things we'll cover in today's lecture: \n",
    "\n",
    "- Something\n",
    "- Something else\n",
    "- OMG I don't believe it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear regression - main idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's say we have the following graph: \n",
    "\n",
    "<img align=\"middle\" src=\"img/scatter_plot_data_only.png\" width=\"500px\" style=\"padding:30px;border:thin solid white;\"> \n",
    "\n",
    "Given outcomes (targets) $y^{(i)} \\in \\mathbb{R}$ for covariates (inputs) $x^{(i)} \\in \\mathbb{R}$, what is/are the underlying system model and/or the parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Multiple possible solutions to the data: \n",
    "\n",
    "<img align=\"middle\" src=\"img/scatter_plot_with_sample_lines.png\" width=\"500px\" style=\"padding:30px;border:thin solid white;\"> \n",
    "\n",
    "Visually, the best fit is obvious. But how do we communicate the best fit to a computer?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 10 x values between 0 and 10\n",
    "x = np.linspace(0, 10, 10)\n",
    "\n",
    "# Generate y values with some variability around the line y = 2x + 5\n",
    "y = 2 * x + 5 + np.random.normal(0, 2, size=len(x))\n",
    "\n",
    "# Create the figure and axis\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Scatter plot of generated points\n",
    "plt.scatter(x, y, label='Generated Data', color='blue')\n",
    "\n",
    "# Plot the line y = 2x + 5\n",
    "plt.plot(x, 2 * x + 5, label='y = 2x + 5', color='blue', linewidth=2)\n",
    "plt.plot(x, 1 * x + 8, label='y = 1x + 7', color='red', linewidth=2)\n",
    "plt.plot(x, 6 * x + 0, label='y = 6x + 0', color='green', linewidth=2)\n",
    "plt.plot(x, 0 * x + 15, label='y = -0x + 15', color='purple', linewidth=2)\n",
    "plt.plot(x, 0.5 * x + 10, label='y = 0.5x + 10', color='orange', linewidth=2)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.ylim(0, 30)\n",
    "#plt.title('Scatter Plot with Best Fit Line (Seed = 42)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig(\"./img/scatter_plot_with_sample_lines.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Close the plot to avoid displaying it\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's assume the best model is linear: \n",
    "\n",
    "$$ y = wx + b $$\n",
    "\n",
    "where $y$ is the prediction, $w$ is the weight(s), and $b$ is the bias\n",
    "\n",
    "$w$ and $b$ are the parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How do we measure how far (or close) one set of parameters is to the data? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Database notation\n",
    "\n",
    "Quick note about notation for data: \n",
    "\n",
    "- Given a dataset ${\\cal D} = \\{(x^{(i)},y^{(i)})\\}_{i=1}^N$\n",
    "- Data $x^{(i)} \\in \\mathbb{R}^d$\n",
    "- **Question:** Given new unseen data $ùë•$ how to predict its label $ùë¶$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss function - least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "The first error we'll discuss is the **squared error** loss: \n",
    "\n",
    "$$ \\mathcal{L}(y,t) = \\frac{1}{2}\\left( y-t \\right)^2 $$ \n",
    "\n",
    "- $y-t$ is called the residual (see below) and we want to make it as small as possible\n",
    "- $y$ is the predicted value and $t$ is the ground truth\n",
    "- the \\frac{1}{2} factor is just to make our gradient calculation cleaner later, it doesn't really affect anything\n",
    "\n",
    "<img align=\"middle\" src=\"img/scatter_plot_with_residuals.png\" width=\"500px\" style=\"padding:30px;border:thin solid white;\"> \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 10 x values between 0 and 10\n",
    "x = np.linspace(0, 10, 10)\n",
    "\n",
    "# Generate y values with some variability around the line y = 2x + 5\n",
    "y = 2 * x + 5 + np.random.normal(0, 2, size=len(x))\n",
    "\n",
    "# Create the figure and axis\n",
    "plt.figure(figsize=(4, 3))\n",
    "\n",
    "# Scatter plot of generated points\n",
    "plt.scatter(x, y, label='Generated Data', color='blue')\n",
    "\n",
    "# Plot the line y = 2x + 5\n",
    "plt.plot(x, 2 * x + 5, label='y = 2x + 5', color='blue', linewidth=2)\n",
    "\n",
    "for xi, yi in zip(x, y):\n",
    "    plt.plot([xi, xi], [yi, 2 * xi + 5], color='red', linestyle='dashed')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.ylim(0, 30)\n",
    "#plt.title('Scatter Plot with Best Fit Line (Seed = 42)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig(\"./img/scatter_plot_with_residuals.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Close the plot to avoid displaying it\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Cost function**: loss function averaged over all training examples\n",
    "\n",
    "$$\n",
    "\\mathcal{E}(w, b) = \\frac{1}{2N} \\sum_{i=1}^{N} \\left( y^{(i)} - t^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{2N} \\sum_{i=1}^{N} \\left( wx^{(i)} + b - t^{(i)} \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Depending on the value of $w$/$b$ we choose, the loss can differ significantly: \n",
    "\n",
    "<img align=\"left\" src=\"img/scatter_plot_with_sample_lines.png\" width=\"450px\" style=\"padding:30px;border:thin solid white;\"> <img align=\"right\" src=\"img/loss_contour_with_guesses.png\" width=\"450px\" style=\"padding:30px;border:thin solid white;\"> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 10 x values between 0 and 10\n",
    "x = np.linspace(0, 10, 10)\n",
    "\n",
    "# Generate true target values with some noise\n",
    "true_m = 2\n",
    "true_b = 5\n",
    "t = true_m * x + true_b + np.random.normal(0, 2, size=len(x))\n",
    "\n",
    "# Define ranges for m and b\n",
    "m_values = np.linspace(-4, 8, 100)\n",
    "b_values = np.linspace(-10, 20, 100)\n",
    "\n",
    "# Create a meshgrid for contour plot\n",
    "M, B = np.meshgrid(m_values, b_values)\n",
    "\n",
    "# Compute loss for each combination of m and b\n",
    "loss = np.zeros_like(M)\n",
    "\n",
    "for i in range(M.shape[0]):\n",
    "    for j in range(M.shape[1]):\n",
    "        y_pred = M[i, j] * x + B[i, j]\n",
    "        loss[i, j] = 0.5 * np.mean((y_pred - t) ** 2)\n",
    "\n",
    "# Create contour plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "contour = plt.contour(M, B, loss, levels=50, cmap='viridis')\n",
    "plt.clabel(contour, inline=True, fontsize=8)\n",
    "plt.xlabel(\"Slope (m)\")\n",
    "plt.ylabel(\"Intercept (b)\")\n",
    "#plt.title(\"Contour Plot of Loss Function\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Add specific points with given colors\n",
    "points = [(2, 5, 'blue'), (1, 7, 'red'), (6, 0, 'green'), (0, 15, 'purple'), (0.5, 10, 'orange')]\n",
    "for m, b, color in points:\n",
    "    plt.scatter(m, b, color=color, edgecolors='black', s=100, label=f'({m},{b})')\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig(\"./img/loss_contour_with_guesses.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solving the optimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So we have the loss function: \n",
    "\\begin{align}\n",
    "\\mathcal{E}(w_1, \\dots, w_D, b) &= \\frac{1}{N} \\sum_{i=1}^{N} L(y^{(i)}, t^{(i)})\\\\\n",
    "                                &= \\frac{1}{2N} \\sum_{i=1}^{N} \\left( y^{(i)} - t^{(i)} \\right)^2\\\\\n",
    "                                &= \\frac{1}{2N} \\sum_{i=1}^{N} \\left( \\sum_{j} w_j x_j^{(i)} + b - t^{(i)} \\right)^2\n",
    "\\end{align}\n",
    "Now we need the partial derivatives $\\left(\\frac{\\partial \\mathcal{E}}{\\partial w}\\right)$ and $\\left(\\frac{\\partial \\mathcal{E}}{\\partial b}\\right)$ to solve for the parameters $w$ and $b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Direct Solution\n",
    "\n",
    "---\n",
    "Let's first simplify the linear equation: \n",
    "\n",
    "Instead of $y=wx+b$, let's say $y=w_1x_1 + w_0x_0$ where $x_1$ is the original data point and $x_0=1$ always. Doign things this way allows us to write: $y=WX$ where $W = \\left[w_0, w_1\\right]$ and $X = \\left[1, x_1\\right]$\n",
    "\n",
    "---\n",
    "\n",
    "Writing hings this way helps us simplyfy our equation:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{E}(w_1, \\dots, w_D, b) &= \\frac{1}{N} \\sum_{i=1}^{N} L(y^{(i)}, t^{(i)})\\\\\n",
    "                                &= \\frac{1}{2N} \\sum_{i=1}^{N} \\left( y^{(i)} - t^{(i)} \\right)^2\\\\\n",
    "                                &= \\frac{1}{2N} \\sum_{i=1}^{N} \\left( \\sum_{j} w_j x_j^{(i)} - t^{(i)} \\right)^2\n",
    "\\end{align}\n",
    "\n",
    "--- \n",
    "\n",
    "So the partial derivatives of this equation will be: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{E}}{\\partial w_j} = \\frac{1}{N} \\sum_{i=1}^{N} x_j^{(i)} \\left( y^{(i)} - t^{(i)} \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "So we have:\n",
    "\n",
    "So the partial derivatives of these equations will be: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{E}}{\\partial w_j} = \\frac{1}{N} \\sum_{i=1}^{N} x_j^{(i)} \\left( y^{(i)} - t^{(i)} \\right)\n",
    "$$\n",
    "\n",
    "which when we expand it out will be: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{E}}{\\partial w_j} = \\frac{1}{N} \\sum_{i=1}^{N} x_j^{(i)} \\left( \\sum_{j'=1}^{D} w_{j'} x_{j'}^{(i)} - t^{(i)} \\right)\n",
    "$$\n",
    "\n",
    "Notice that if we want to solve for $w$ we need to set the partial derivative to zero. Doing that and with a bit of reorganization we can get: \n",
    " \n",
    "$$\n",
    "\\frac{\\partial \\mathcal{E}}{\\partial w_j} = \\frac{1}{N} \\sum_{j'=1}^{D} \\left( \\sum_{i=1}^{N} x_j^{(i)} x_{j'}^{(i)} \\right) w_{j'} - \\frac{1}{N} \\sum_{i=1}^{N} x_j^{(i)} t^{(i)} = 0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Descent\n",
    "\n",
    "How would we solve this using gradient descent? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Same way we always calculate things with gradient descent: \n",
    "\n",
    "$$\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\frac{\\partial \\mathcal{E}}{\\partial \\mathbf{w}},\n",
    "$$\n",
    "\n",
    "which in our case becomes: \n",
    "\n",
    "$$\n",
    "w_j \\leftarrow w_j - \\alpha \\frac{1}{N} \\sum_{i=1}^{N} x_j \\left( y^{(i)} - t^{(i)} \\right)\n",
    "$$\n",
    "\n",
    "let's test it out!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dataset we are artificially creating: \n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 10 x values between 0 and 10\n",
    "N=10\n",
    "x = torch.linspace(0, 10, N)\n",
    "\n",
    "# Generate true target values with some noise\n",
    "true_m = 2\n",
    "true_b = 5\n",
    "t = torch.tensor(true_m * x + true_b + np.random.normal(0, 2, size=len(x)))\n",
    "\n",
    "X = np.column_stack((np.ones(10), x))\n",
    "\n",
    "# initialize guesses for a, b, and c\n",
    "w_gd = torch.randn((2), requires_grad=True) # size (1,)\n",
    "print('Initial guesses: w0={:.6f}, w1={:.6f}'.format(w_gd[0].data, w_gd[1].data))\n",
    "\n",
    "# information for tracking\n",
    "b_vals = [w_gd[0].data.item()]\n",
    "w_vals = [w_gd[1].data.item()]\n",
    "\n",
    "# gradient descent loop\n",
    "n_iter = 10000 # number of iterations\n",
    "alpha = 10e-3 # step size\n",
    "for n in range(n_iter):\n",
    "    with torch.no_grad():\n",
    "        # don't want the gradient update step to accumulate further gradients at a, b, and c\n",
    "        y = w_gd[1]*x+w_gd[0]\n",
    "        w_gd -= alpha*(1/N)*sum(x*(y-t))\n",
    "        # manually zero out the gradients before next backward pass\n",
    "        w_gd.grad = None\n",
    "        \n",
    "    # log information\n",
    "    w_vals.append(w_gd[1].data.item())\n",
    "    b_vals.append(w_gd[0].data.item())\n",
    "    \n",
    "# examine solution\n",
    "print('Final guesses: w0={:.6f}, w1={:.6f}'.format(w_gd[0].data, w_gd[1].data))\n",
    "\n",
    "iter_num = np.array([0, 50, 100, 250, 500]).astype(int)\n",
    "plt.figure(figsize=(20, 5))\n",
    "for j, i in enumerate(iter_num):\n",
    "    plt.subplot(1, 5, j+1)\n",
    "    curr_fn = w_vals[i]*x + b_vals[i]\n",
    "    plt.plot(x.detach().numpy(), curr_fn.detach().numpy(), color='blue')\n",
    "    plt.scatter(x.detach().numpy(), t, color='orange')\n",
    "    plt.grid(True)\n",
    "    plt.title('Regressed function: Iteration {}'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Speaking of numerial simulations, do we really need to even calculate the gradient? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dataset we are artificially creating: \n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 10 x values between 0 and 10\n",
    "N=10\n",
    "x = torch.linspace(0, 10, N)\n",
    "\n",
    "# Generate true target values with some noise\n",
    "true_m = 2\n",
    "true_b = 5\n",
    "t = torch.tensor(true_m * x + true_b + np.random.normal(0, 2, size=len(x)))\n",
    "\n",
    "X = np.column_stack((np.ones(10), x))\n",
    "\n",
    "# initialize guesses for a, b, and c\n",
    "w_gd = torch.randn((2), requires_grad=True) # size (1,)\n",
    "print('Initial guesses: w0={:.6f}, w1={:.6f}'.format(w_gd[0].data, w_gd[1].data))\n",
    "\n",
    "# information for tracking\n",
    "b_vals = [w_gd[0].data.item()]\n",
    "w_vals = [w_gd[1].data.item()]\n",
    "loss_vals = []\n",
    "\n",
    "# gradient descent loop\n",
    "n_iter = 5000 # number of iterations\n",
    "alpha = 10e-3 # step size\n",
    "for n in range(n_iter):\n",
    "    # compute loss function (objective function)\n",
    "    errors = t-(w_gd[0] + w_gd[1]*x)\n",
    "    loss = torch.sum((errors)**2)/N\n",
    "    # backpropagate gradients\n",
    "    loss.backward()\n",
    "    # perform gradient descent update step\n",
    "    with torch.no_grad():\n",
    "        # don't want the gradient update step to accumulate further gradients at a, b, and c\n",
    "        w_gd -= alpha*w_gd.grad\n",
    "        # manually zero out the gradients before next backward pass\n",
    "        w_gd.grad = None\n",
    "        \n",
    "    # log information\n",
    "    loss_vals.append(loss.item()) # log MSE\n",
    "    w_vals.append(w_gd[1].data.item())\n",
    "    b_vals.append(w_gd[0].data.item())\n",
    "    \n",
    "# examine solution\n",
    "print('Final guesses: w0={:.6f}, w1={:.6f}'.format(w_gd[0].data, w_gd[1].data))\n",
    "\n",
    "# visualize loss and progression of solution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogy(loss_vals, color='blue')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('MSE value for regression')\n",
    "\n",
    "iter_num = np.array([0, 50, 100, 250, 5000]).astype(int)\n",
    "plt.figure(figsize=(20, 5))\n",
    "for j, i in enumerate(iter_num):\n",
    "    plt.subplot(1, 5, j+1)\n",
    "    curr_fn = w_vals[i]*x + b_vals[i]\n",
    "    plt.plot(x.detach().numpy(), curr_fn.detach().numpy(), color='blue')\n",
    "    plt.scatter(x.detach().numpy(), t, color='orange')\n",
    "    plt.grid(True)\n",
    "    plt.title('Regressed function: Iteration {}'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Matrix Solution\n",
    "\n",
    "---\n",
    "Let's revisit this equation: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{E}}{\\partial w_j} = \\frac{1}{N} \\sum_{j'=1}^{D} \\left( \\sum_{i=1}^{N} x_j^{(i)} x_{j'}^{(i)} \\right) w_{j'} - \\frac{1}{N} \\sum_{i=1}^{N} x_j^{(i)} t^{(i)} = 0\n",
    "$$\n",
    "\n",
    "--- \n",
    "\n",
    "We can transform this equation into the vector form: \n",
    "\n",
    "$$\n",
    "X^T X W - X^T T = 0\n",
    "$$\n",
    "\n",
    "hence\n",
    "\n",
    "$$\n",
    "X^T X W = X^T T \n",
    "$$\n",
    "\n",
    "Then we multiply both sides by $(X^T X)^{-1}$ to get: \n",
    "\n",
    "$$\n",
    "W = (X^T X)^{-1} X^T T \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 10 x values between 0 and 10\n",
    "x = np.linspace(0, 10, 10)\n",
    "\n",
    "# Generate true target values with some noise\n",
    "true_m = 2\n",
    "true_b = 5\n",
    "t = true_m * x + true_b + np.random.normal(0, 2, size=len(x))\n",
    "#t = np.array([5.99342831,  6.94569362, 10.73982152, 14.71272638, 13.42058214, 15.6428372, 21.49175896, 22.09042501, 21.83882901, 26.08512009])\n",
    "\n",
    "print(x)\n",
    "print(t)\n",
    "X = np.column_stack((np.ones(10), x))\n",
    "\n",
    "W = np.linalg.inv(X.transpose()@X)@X.transpose()@t\n",
    "\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we had the matrix solution, why would you even bother with gradient descent? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Linear regression seems pretty great. Why would we bother with anything? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Linear regression has its limits: \n",
    "\n",
    "<img align=\"center\" src=\"./img/scatter_plot_with_sample_linear_fail.png\" width=\"500px\" style=\"padding:30px;border:thin solid white;\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 10 x values between 0 and 10\n",
    "x = np.linspace(-10, 10, 10)\n",
    "\n",
    "# Generate y values with some variability\n",
    "def f(x): \n",
    "    return 0.25 * x **3 + -0.5 * x**2 + -10 * x + 20 \n",
    "\n",
    "y = f(x) + np.random.normal(0, 15, size=len(x))\n",
    "\n",
    "\n",
    "# Create the figure and axis\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Scatter plot of generated points\n",
    "plt.scatter(x, y, label='Generated Data', color='blue')\n",
    "\n",
    "# Plot the line\n",
    "px = np.linspace(-10, 10, 100)\n",
    "plt.plot(px,-1*px+10, label='y = 2x + 5', color='blue', linewidth=2)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "#plt.ylim(0, 30)\n",
    "#plt.title('Scatter Plot with Best Fit Line (Seed = 42)')\n",
    "#plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig(\"./img/scatter_plot_with_sample_linear_fail.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## That's it for today\n",
    "\n",
    "- Have a good weekend!\n",
    "- Don't forget HW3 is due Monday."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Additional References\n",
    "\n",
    "[1] Roger Grosse CSC321 lectures - https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
