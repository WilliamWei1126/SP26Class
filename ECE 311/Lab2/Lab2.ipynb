{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE 311 Lab 2: LSI Systems\n",
    "## Due Date: 2/18 @ 11:59PM on Canvas\n",
    "In this lab, we will explore Linear Shift-Invariant (LSI) systems and their properties with applications involving toy signals, image filtering, stock data, and even an example of a simple non-linear system. Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries for this lab\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.io import imread\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with Convolution\n",
    "\n",
    "We remember from ECE 210, that convolution describes how any continuous-time input signal is processed by an LTI system. Given an input $x(t)$ and an LTI system's impulse response $h(t)$, the system output $y(t)$ is given by\n",
    "\n",
    "$$\n",
    "y(t) = x(t) * h(t).\n",
    "$$\n",
    "\n",
    "Recall that convolution for continous-time signals is defined as\n",
    "\n",
    "$$\n",
    "y(t) = \\int_{\\tau = -\\infty}^{\\infty}x(\\tau)h(t-\\tau)d\\tau = \\int_{\\tau=-\\infty}^{\\infty}x(t-\\tau)h(\\tau)d\\tau.\n",
    "$$\n",
    "\n",
    "You have learned in ECE 310 that discrete-time LTI systems also have an impulse response $h[n]$, which is the system response to a unit Kronecker delta $\\delta[n]$ input. Thus, we can express the system output given an input signal via discrete-time convolution.\n",
    "\n",
    "$$\n",
    "y[n] = x[n] * h[n]\n",
    "$$\n",
    "\n",
    "$$\n",
    "y[n] = \\sum_{k=-\\infty}^{\\infty}x[k]h[n-k] = \\sum_{k=-\\infty}^{\\infty}x[n-k]h[k]\n",
    "$$\n",
    "\n",
    "Like the width properties of continuous-time convolution, if $x$ is of length $N$ and $h$ is of length $M$, the result $y$ will be of length $N+M-1$. It is important to note that every LTI system can be represented by a convolution, every system that can be expressed as a convolution is fully described by its impulse response, and any system fully described by its impulse response must be LTI. This means the relationship between LTI systems, convolution, and impulse responses is an \"if and only if\" relationship; they all imply one another! This is something handy to keep in mind whenever you want to identify and describe an LTI system.\n",
    "\n",
    "The key function we will use to perform convolutions is the $\\textrm{convolve()}$ function in the $\\textrm{scipy.signal}$ module. The usage of this function for an example system is as follows:\n",
    "\n",
    "$$\n",
    "x[n] = \\delta[n]+2\\delta[n-2]+3\\delta[n-4]\n",
    "$$\n",
    "\n",
    "$$\n",
    "y[n] = x[n]+3x[n-1]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 0, 2, 0, 3]) # input signal\n",
    "h = np.array([1, 3]) # filter/system's impulse response\n",
    "y = signal.convolve(x,h) # signal.convolve(in1,in2)\n",
    "\n",
    "print(y) # verify this result by hand!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we extracted the system's impulse response for the system's Linear Constant Coefficient Difference Equation (LCCDE): we simply replace each $x[n-k]$ with $\\delta[n-k]$! This is equivalent to passing $\\delta[n]$ as our input signal. Above, our first term takes the present input value and multiplies it by one, and the second term multiplies the most recent input by three. Intuitively, when we flip and shift our filter $h$ for the convolution, we will be applying this system to the input signal at each shift step. The $\\textrm{signal.convolve()}$ function assumes the arrays that represent our signals begin at index zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Implementing LTI Systems\n",
    "\n",
    "In the below code cell, implement the following LTI systems and plot the system response (using $\\textrm{plt.stem()}$) to each of the listed input signals. Comment on the results in the following Markdown cell, i.e. compare and contrast how each system seems to affect each input signal. Remember to use the LCCDE for each system to infer its impulse response!\n",
    "\n",
    "* System A: $y_a[n] = x[n]-x[n-1]$\n",
    "\n",
    "\n",
    "* System B: $y_b[n] = \\frac{1}{3}x[n]+\\frac{1}{3}x[n-1]+\\frac{1}{3}x[n-2]$\n",
    "\n",
    "\n",
    "\n",
    "* $x_1[n] = u[n] - u[n-10], 0\\leq n< 20$\n",
    "\n",
    "\n",
    "* $x_2[n] = \\sin\\left(\\frac{\\pi}{20}n\\right), 0\\leq n< 40$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input signals here:\n",
    "# Hint: Use np.sin and np.pi!\n",
    "x1 = np.array([1 for i in range(10)] + [0 for i in range(10)])\n",
    "x2 = np.array([np.sin(np.pi/20 * i) for i in range (40)])\n",
    "# System A\n",
    "# plot result for x1\n",
    "x1A = x1 - np.concatenate((np.array([0]), x1[:-1]), axis = 0)\n",
    "n = np.array([i for i in range(x1A.size)])\n",
    "plt.figure(figsize = (12,8))\n",
    "plt.subplot(121)\n",
    "plt.stem(n, x1A)\n",
    "plt.title('x1 after system A')\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('y[n]')\n",
    "# plot result for x2\n",
    "x2A = x2 - np.concatenate((np.array([0]), x2[:-1]), axis = 0)\n",
    "n = np.array([i for i in range(x2A.size)])\n",
    "plt.subplot(122)\n",
    "plt.stem(n, x2A)\n",
    "plt.title('x2 after system A')\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('y[n]')\n",
    "\n",
    "# System B\n",
    "# plot result for x1\n",
    "x1B = (1/3)*x1 + (1/3)*np.concatenate((np.array([0]), x1[:-1]), axis = 0) + (1/3)*np.concatenate((np.array([0,0]), x1[:-2]), axis = 0)\n",
    "n = np.array([i for i in range(x1B.size)])\n",
    "plt.figure(figsize = (12,8))\n",
    "plt.subplot(121)\n",
    "plt.stem(n, x1B)\n",
    "plt.title('x1 after system B')\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('y[n]')\n",
    "# plot result for x2\n",
    "x2B = (1/3)*x2 + (1/3)*np.concatenate((np.array([0]), x2[:-1]), axis = 0) + (1/3)*np.concatenate((np.array([0,0]), x2[:-2]), axis = 0)\n",
    "n = np.array([i for i in range(x2B.size)])\n",
    "plt.subplot(122)\n",
    "plt.stem(n, x2B)\n",
    "plt.title('x2 after system B')\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('y[n]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments here:\n",
    "\n",
    "(Consider how each system affects the flatter and faster-changing parts of the input signals. What do you think each system is doing?)\n",
    "I think system A is differentiating the signal since by doing x[n] - x[n-1], we are calculating the change from one term to another. On the other hand, System B is averaging the closest 3 values, so I think System B smooths the input. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Bitcoin Pricing Correction\n",
    "\n",
    "One type of signal could be some non-physical 1D information. An example of this would be stock price or cryptocurrency price data. This data is notoriously noisy and can jump around unpredictably.\n",
    "\n",
    "Remember that the systems we work with can be either causal or non-causal. A causal system only uses present and past information or values to calculate its present values, while a non-causal system can leverage future information. In this exercise, we will compare causal and non-causal versions of a system to smooth a day's worth of bitcoin price data. We have provided 24 hours of prices with pricing updates every minute (1440 points). The date in question is Christmas Eve Day (12/24), 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin_data = np.load('bitcoin-christmas.npy', allow_pickle=True)\n",
    "n_points = len(bitcoin_data)\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.plot(range(n_points), bitcoin_data)\n",
    "plt.title('Bitcoin Prices Every Minute 12/24/2017')\n",
    "plt.xlabel('Minute from Midnight')\n",
    "plt.ylabel('Price (USD)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty noisy, right? Maybe a lot of last-minute Christmas gifts made the price even more unpredictable!\n",
    "\n",
    "In this exercise, you will implement two length-51 moving average filters on this Bitcoin price data. The first will be causal and the second will be non-causal. Mathematically, we can represent these systems as follows:\n",
    "\n",
    "$$\n",
    "y_1[n] = \\frac{1}{51}\\sum_{i=0}^{50}x[n - i]\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_2[n] = \\frac{1}{51}\\sum_{i=-25}^{25}x[n - i]\n",
    "$$\n",
    "\n",
    "Furthermore, since the moving average filter is an LTI system we may implement it as a convolution using its impulse response. If you are having trouble seeing this, we suggest considering a length-5 moving average filter and \"unrolling\" the sum to understand that system definition and its impulse response.\n",
    "\n",
    "Notice that the non-causal filter will require us to access negative indices according to the impulse response of our filter. A natural question to ask is how does the $\\textrm{signal.convolve()}$ function perform non-causal convolution? How can you indicate negative indices when making an array for a system's impulse response? This is where the \"same\" mode comes in! We may use the \"same\" mode as follows:\n",
    "\n",
    "```\n",
    "y = signal.convolve(x, h, 'same'),\n",
    "```\n",
    "\n",
    "where $x$ is of length $N$ and $h$ is length $M$. This line of code will first perform regular convolution like the default mode where the first sample of each sequence is assumed to be at $n=0$. Then, it will only keep the center $N$ values (length of first argument/array). This operation is equivalent to zero-centering the second argument/array (```h``` in the above example). You may want to try a couple small examples to convince yourself this is true. The \"same\" mode will be important to keep in mind throughout this lab and the rest of the course.\n",
    "\n",
    "**Important Note:** For the following two parts, we have provided the appropriate start and end indices to help us make sure each implementation returns results of the same size and to remove initial condition worries (ramping behavior since we would have fewer than 51 samples as the filter has partial overlap). **Please follow the plotting instructions in the following parts carefully!**\n",
    "\n",
    "a. Construct the causal filter and apply it to the provided bitcoin price data (apply your causal system to the data in the ``bitcoin_data`` variable). To make sure your output is the same length and matches up correctly in time, please slice the result of convolving ``bitcoin_data`` with your causal filter using ``start`` and ``end``. Note that we do this to create the ``plotting_data`` variable. Plot the original data (``plotting_data``) and your smoothed data on the same plot. **Don't forget to include a legend!**\n",
    "\n",
    "b. Construct the non-causal filter and apply it to the provided bitcoin price data (``bitcoin_data`` variable). Perform the same ``start`` and ``end`` slicing on your result as in part (a). Plot the original data (``plotting_data``) and your smoothed data on the same plot.\n",
    "\n",
    "c. Plot the difference signals for each filter on the same plot. Let the difference signal for a system's output be given by\n",
    "\n",
    "$$\n",
    "y_d = y - \\hat{y},\n",
    "$$\n",
    "\n",
    "where $\\hat{y}$ is your system output and $y$ is the sliced original data used for plotting, ``plotting_data``.\n",
    "\n",
    "d. Comment on the results in the following Markdown cell. What is noticeably different between the two sets of smoothed results from the causal and non-causal systems? Is it helpful to know a lot of past information or a decent amount of past and future information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided code\n",
    "L = 51\n",
    "half_L = 25\n",
    "start = 50\n",
    "end = len(bitcoin_data)-half_L\n",
    "plotting_data = bitcoin_data[start:end] # plot against this data in parts a/b, pay attention to how we slice\n",
    "result_length = len(plotting_data)\n",
    "\n",
    "# Code for 2.a here, don't forget to plot original and filtered signals on same plot!\n",
    "ha = np.ones(51) / 51\n",
    "y = signal.convolve(bitcoin_data, ha)\n",
    "n = np.arange(start, end)\n",
    "plt.figure(figsize=(6,8))\n",
    "plt.plot(n, y[start:start+result_length], label = 'after filter (causal)')\n",
    "plt.plot(n, plotting_data, label = 'before filter')\n",
    "plt.title('Bitcoin prices: original vs causal filter')\n",
    "plt.xlabel('Minute from midnight')\n",
    "plt.ylabel('Price(USD)')\n",
    "plt.legend()\n",
    "\n",
    "# Code for 2.b here\n",
    "y2 = signal.convolve(bitcoin_data, ha, 'same')\n",
    "plt.figure(figsize=(6,8))\n",
    "plt.plot(n, y2[start:end], label = 'after filter (non-causal)')\n",
    "plt.plot(n, plotting_data, label = 'before filter')\n",
    "plt.title('Bitcoin prices: original vs non-causal filter')\n",
    "plt.xlabel('Minute from midnight')\n",
    "plt.ylabel('Price(USD)')\n",
    "plt.legend()\n",
    "\n",
    "        \n",
    "    \n",
    "# Code for 2.c here\n",
    "dy = plotting_data - y[start:start+result_length]\n",
    "dy2 = plotting_data - y2[start:end]\n",
    "plt.figure(figsize=(6,8))\n",
    "plt.plot(n, dy, label = 'original - causal')\n",
    "plt.plot(n, dy2, label = 'original - non-causal')\n",
    "plt.title('Differences of original data vs filtered data')\n",
    "plt.xlabel('Minute from midnight')\n",
    "plt.ylabel('Difference (USD)')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments for here:\n",
    "\n",
    "Part 2(d):\n",
    "As seen in the differences part, the non-causal filter performs better than the causal filter, as clearly seen in the differences graph. The casual filter uses the past 50 samples, so the curve doesn't align directly with the original signal; it is shifted a little bit. On the other hand, the non-casual one averages 25 signals in the past and future, so its prediction is much more reliable, and the data is not shifted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ECE 310, we typically focus on the implications of applying LTI systems in the frequency domain of a 1D signal. Our most common idea of a 1D signal is a piece of audio. In this section, we will experiment with image filtering along both axes of an image and see that we can do more than just filtering with convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: 1D Image Convolution\n",
    "\n",
    "Apply a 1D length 11 moving average filter on the provided ``test-image.jpg`` image along the:\n",
    "\n",
    "**Note: we will use the 'same' mode again when using $\\textrm{signal.convolve()}$ for this exercise.**\n",
    "\n",
    "a. Rows \n",
    "\n",
    "b. Columns\n",
    "\n",
    "c. Rows then columns (**Hint**: make sure to convolve first along the rows, then use the result of this row-convolution to convolve along the columns!)\n",
    "\n",
    "d. Columns then rows (**Hint**: make sure to convolve first along the columns, then use the result of this column-convolution to convolve along the rows!)\n",
    "\n",
    "Plot each of the resulting images and give them unique titles.\n",
    "\n",
    "e. Comment on the images from the \"rows then columns\" and \"columns then rows\" procedures. Are they the same? Explain your answer, why are they the same or different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make filter and load image\n",
    "image = imread('test-image.jpg')\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(image, 'gray')\n",
    "plt.title('Original Image')\n",
    "L = 11\n",
    "h = np.ones(L) * 1/11 # impulse response of length-11 moving average filter\n",
    "n_rows, n_cols = image.shape\n",
    "# Code for 3.a along rows (apply filter to each row independently)\n",
    "image_row = np.zeros(image.shape)\n",
    "# Hint: image_row[i, :] = signal.convolve(image[i, :], h, 'same') for each row\n",
    "for i in range(image_row.shape[0]):\n",
    "    image_row[i, :] = signal.convolve(image[i, :], h, 'same')\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(image_row, 'gray')\n",
    "plt.title('Convolve along x axis Image')\n",
    "# along the columns (3.b)\n",
    "image_col = np.zeros(image.shape)\n",
    "for i in range(image_col.shape[1]):\n",
    "    image_col[:, i] = signal.convolve(image[:, i], h, 'same')\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(image_col, 'gray')\n",
    "plt.title('Convolve along y axis Image')\n",
    "# rows then columns (3.c)\n",
    "image_rowcol = np.zeros(image.shape)\n",
    "for i in range(image_rowcol.shape[0]):\n",
    "    image_rowcol[i, :] = signal.convolve(image[i, :], h, 'same')\n",
    "for i in range(image_rowcol.shape[1]):\n",
    "    image_rowcol[:, i] = signal.convolve(image_rowcol[:, i], h, 'same')\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(image_rowcol, 'gray')\n",
    "plt.title('Convolve along x and then y axis Image')\n",
    "# columns then rows (3.d)\n",
    "image_colrow = np.zeros(image.shape)\n",
    "for i in range(image_colrow.shape[1]):\n",
    "    image_colrow[:, i] = signal.convolve(image[:, i], h, 'same')\n",
    "for i in range(image_colrow.shape[0]):\n",
    "    image_colrow[i, :] = signal.convolve(image_colrow[i, :], h, 'same')\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(image_colrow, 'gray')\n",
    "plt.title('Convolve along y and then x axis Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments here\n",
    "\n",
    "Part 3(e):\n",
    "Both convolving along the x and then y axis and y and then x axis should be the same as seen in the pictures. They should be the same because convolution averages the 11 pixels along the x-axis first, and then averages the 11 pixels along the y-axis, which effectively averages the 11 by 11 area around each pixel. Therefore, flipping the sequence to the y-axis first and then the x-axis should have the same effect as averaging the 11*11 area. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Building an Edge Detector\n",
    "\n",
    "Let's now apply image convolution to perform edge detection. We will build a simple edge detector step-by-step using the following simple 1D filter:\n",
    "\n",
    "$$\n",
    "h[n] = \\delta[n] - \\delta[n-1]\n",
    "$$\n",
    "\n",
    "a. Intuitively or mathematically, what does this filter do to an input signal? In other words, what parts of a signal would give a strong (large magnitude) response and what parts would give a weak (small magnitude) response? You may answer this with a couple signal examples and the result of convolution with $h[n]$ or qualitative intuition.\n",
    "\n",
    "b. Is this filter causal? Why or why not? In general, is it a problem if an image filter is non-causal? (Hint: consider the contexts in which we can or cannot violate causality.)\n",
    "\n",
    "**Note**: For the next two parts, please store your results in separate variables. This will make part (e) much cleaner.\n",
    "\n",
    "c. Apply $h[n]$ along the rows of the ``test-image.jpg`` image. Plot the result with a grayscale color mapping.\n",
    "\n",
    "d. Apply $h[n]$ along the columns of the ``test-image.jpg`` image. Plot the result with a grascale color mapping.\n",
    "\n",
    "So far we have checked for edge-like features in the image going along the rows and columns. Imagine these two results as indicating edge strength along the row axis (vertical edges) and column axis (horizontal edges) of the image, respectively. Take a minute to look at the differences between these two resulting images. Can you tell which one is detecting edges within a row and which one is doing so within a column? What would be a sensible way to incorporate these two dimensions of information? Imagine they form a 2D vector and take the norm! More precisely:\n",
    "\n",
    "$$\n",
    "I_F(r,c) = \\sqrt{\\left(I_R(r,c)\\right)^2 + \\left(I_C(r,c)\\right)^2},\n",
    "$$\n",
    "\n",
    "where $I_R$ and $I_C$ are the row and column filtered results from parts (c) and (d) above, respectively.\n",
    "\n",
    "e. Build the final result image $I_F$ according to the above equation. Plot the result again with a grayscale color mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments here\n",
    " \n",
    "Part 4(a):   \n",
    "This filter compares this signal and the last signal and calculates the differences between them. Effectively, as shown:   \n",
    "y[n] = x[n]-x[n-1]  \n",
    "This means that this signal would give a strong response when the change is big and a small response when the change is small. Acting like a differential in calculus.   \n",
    "for example x[n] = [1 1 9]   \n",
    "The result y[n] = [1,0,8], where 8 represents the huge jump from 1 to 9   \n",
    "\n",
    "Part 4(b):   \n",
    "This signal is causal because it only requires the current and past values. It is not a problem for an image filter to be non-causal because we are not actually dealing with time, we don't need to forsee future. However, in some special cases, such as real-time streaming, it might become an issue where we cannot process non casual filter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test-image.jpg\n",
    "test = imread('test-image.jpg')\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(test, 'gray')\n",
    "plt.title('Original photo')\n",
    "h = np.array([1, -1])\n",
    "# Code for 4.c here:\n",
    "test_row = np.zeros(test.shape)\n",
    "for i in range(test.shape[0]):\n",
    "    test_row[i, :] = signal.convolve(test[i, :], h, 'same')\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(test_row, 'gray')\n",
    "plt.title('Convolve along x axis Image')\n",
    "\n",
    "# Code for 4.d here:\n",
    "test_col = np.zeros(test.shape)\n",
    "for i in range(test.shape[1]):\n",
    "    test_col[:, i] = signal.convolve(test[:, i], h, 'same')\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(test_col, 'gray')\n",
    "plt.title('Convolve along y axis Image')\n",
    "\n",
    "\n",
    "# Code for 4.e here:\n",
    "test_final = np.sqrt(test_row**2 + test_col**2)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(test_final, 'gray')\n",
    "plt.title('Image If')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Image Convolution\n",
    "\n",
    "We don't need to limit ourselves to 1D image convolution. Our filters or \"kernels\" can be in two dimensions also! We will not spend much time on the math of 2D convolution/filtering in this class because it is best left for ECE 418 (Image and Video Processing); still,  we can use Python to try it out. But let's try something other than filtering this time!\n",
    "\n",
    "Image convolution is not just for filtering or modifying an image. We can also use convolution to extract information from an image. Remember that convolution is is the process of \"flipping and shifting\" one signal over another signal. At each shift location, we perform a dot product (or inner product) to see how $\\textit{similar}$ the signals are. A larger magnitude value at the output means the two signals are more similar. The following image illustrates 2D convolution.\n",
    "\n",
    "<img src=\"convolution.jpg\">\n",
    "\n",
    "More formally, say we have a $3x3$ convolution kernel $\\mathcal{K}$ where the center pixel is at index $(0,0)$, the result of the 2D convolution at pixel $(i,j)$ for image $I$, $O(i,j)$ is given by:\n",
    "\n",
    "$$\n",
    "O(i,j) = \\sum_{k=-1}^{1}\\sum_{l=-1}^{1}\\mathcal{K}(k,l)I(i-k,j-l)\n",
    "$$\n",
    "\n",
    "Now, why is this useful? Suppose you want to design a system to recognize handwritten digits. How can you tell the difference between a \"1\" and a \"4\", for example? Think about how you as a human can separate these numbers! They both typically have one large vertical line down the middle, but we know we can differentiate them because a \"4\" has another shorter vertical line (depending how you draw it) and a horizontal line connecting them. This is where 2D convolution can help us! How about we create convolution kernels to highlight features we know to be discriminative, like horizontal and vertical lines.\n",
    "\n",
    "The below code cell includes a function to perform 2D image convolution on a target image given a convolution kernel. We have also provided two 2D kernels: one for horizontal features and another for vertical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve_2d(image, kernel):\n",
    "    result = signal.convolve2d(image, kernel, 'same')\n",
    "    result[result < 0] = 0 # Keep values non-negative\n",
    "    return result\n",
    "\n",
    "# identify horizontal lines\n",
    "horiz_kernel = np.array([[-2,-2,-2,-2,-2],\n",
    "                         [1,1,1,1,1],\n",
    "                         [1,1,1,1,1],\n",
    "                         [1,1,1,1,1],\n",
    "                         [-2,-2,-2,-2,-2]])\n",
    "\n",
    "# identify vertical lines\n",
    "vert_kernel = np.array([[-2,1,1,1,-2],\n",
    "                        [-2,1,1,1,-2],\n",
    "                        [-2,1,1,1,-2],\n",
    "                        [-2,1,1,1,-2],\n",
    "                        [-2,1,1,1,-2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the folder for this lab, we have provided example images of the numbers \"1\", \"4\", and \"8\" from the popular MNIST dataset. These images are 28x28 and grayscale. Let's see what our filters can identify in the ``one.jpg`` image! Note the different scales on the feature image colorbars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = imread('one.jpg')\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.subplot(131)\n",
    "plt.title('Original')\n",
    "plt.imshow(one, 'gray')\n",
    "\n",
    "one_horiz = convolve_2d(one, horiz_kernel)\n",
    "plt.subplot(132)\n",
    "plt.title('Horizontal Features')\n",
    "plt.imshow(one_horiz, 'hot')\n",
    "plt.colorbar(fraction=0.05)\n",
    "\n",
    "one_vert = convolve_2d(one, vert_kernel)\n",
    "plt.subplot(133)\n",
    "plt.title('Vertical Features')\n",
    "plt.imshow(one_vert, 'hot')\n",
    "plt.colorbar(fraction=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: 2D Image Convolution for Feature Detection\n",
    "\n",
    "a. Create similar plots as the above example for the \"1\" image for the \"4\" (``four.jpg``) and \"8\" (``eight.jpg``) images in the following code cell.\n",
    "\n",
    "b. Comment on the results and compare what is highlighted for each number.\n",
    "\n",
    "c. What is the significance of having negative kernel values around the positive \"feature highlighting\" values? Think about what would happen if the negative values were zeros instead. Try playing around with the kernels or creating your own kernel if you are unsure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for 5.a here\n",
    "one = imread('four.jpg')\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.subplot(131)\n",
    "plt.title('Original')\n",
    "plt.imshow(one, 'gray')\n",
    "\n",
    "one_horiz = convolve_2d(one, horiz_kernel)\n",
    "plt.subplot(132)\n",
    "plt.title('Horizontal Features')\n",
    "plt.imshow(one_horiz, 'hot')\n",
    "plt.colorbar(fraction=0.05)\n",
    "\n",
    "one_vert = convolve_2d(one, vert_kernel)\n",
    "plt.subplot(133)\n",
    "plt.title('Vertical Features')\n",
    "plt.imshow(one_vert, 'hot')\n",
    "plt.colorbar(fraction=0.05)\n",
    "\n",
    "one = imread('eight.jpg')\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.subplot(131)\n",
    "plt.title('Original')\n",
    "plt.imshow(one, 'gray')\n",
    "\n",
    "one_horiz = convolve_2d(one, horiz_kernel)\n",
    "plt.subplot(132)\n",
    "plt.title('Horizontal Features')\n",
    "plt.imshow(one_horiz, 'hot')\n",
    "plt.colorbar(fraction=0.05)\n",
    "\n",
    "one_vert = convolve_2d(one, vert_kernel)\n",
    "plt.subplot(133)\n",
    "plt.title('Vertical Features')\n",
    "plt.imshow(one_vert, 'hot')\n",
    "plt.colorbar(fraction=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments here:\n",
    "\n",
    "Part 5(b):  \n",
    "In picture 4, the vertical features output shows the vertical lines in the number, and the  horizontal features outputs where the number has horizontal strokes.  \n",
    "In picture 8, similar things happen; the vertical parts of the curve are highlighted in the vertical features graph, and the horizontal parts are highlighted in the horizontal graph.  \n",
    "\n",
    "  \n",
    "Part 5(c):  \n",
    "The negative weight around the edges suppresses the response when the whole area is bright. This makes sure that the horizontal and vertical features can be separated. The horizontal kernel, on the top and bottom, makes sure that vertical strokes won't trigger the response, similar can be said for the vertical kernel. Without the negative values, both kernels will only output positive values, making it hard to distinguish horizontal and vertical features, and it also increases false positives since the edges might not be strong enough to suppress the response. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final activity, we will explore an example of a non-linear system. First, a bit of background.\n",
    "\n",
    "There are many different types of noise that can appear in images. One such type is salt-and-pepper noise. This noise occurs when pixels in a camera or an existing image become fully active or inactive. In other words, a normal pixel either takes on its minimum or maximum possible value. The following code cell shows an original image and a version of it that has been corrupted by 20% salt-and-pepper noise (20% of the pixels are affected). In this activity, we will see whether we can use our LTI systems from before to denoise our image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_image = imread('clean-image.jpg')\n",
    "noisy_image = imread('noisy-image.jpg')\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(121)\n",
    "plt.imshow(clean_image, 'gray')\n",
    "plt.title('Original Image')\n",
    "plt.subplot(122)\n",
    "plt.imshow(noisy_image, 'gray')\n",
    "plt.title('Image with 20% Salt-and-Pepper Noise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6: Non-Linear Systems are Cool Too!\n",
    "\n",
    "We will attempt to use two different filters: a 5x5 mean filter and a 5x5 median filter. Note that a median filter is a non-linear system! A 5x5 median filter simply takes the median of the 25 pixels surrounding the center pixel in the filter and assigns that value to the center pixel.\n",
    "\n",
    "a. Explain/prove why the median filter is a non-linear system. If you don't know where to start, try showing a counter-example for how a one-dimensional median filter fails the test for linearity. \n",
    "\n",
    "b. Apply a 5x5 mean filter to the noisy image and plot the result. You can do this two different ways. You can apply a length-5 mean filter along the rows and columns in any order or use our $\\textrm{convolve_2d()}$ function from before with an appropriate filter you create.\n",
    "\n",
    "c. Apply a 5x5 median filter to the noisy image and plot the result. Use $\\textrm{signal.medfilt()}$ to perform the filtering. Look up the scipy documentation for notes on this function's usage.\n",
    "\n",
    "d. Comment on the differences. Which filter seems to work better? Why do you think so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for 6.b\n",
    "mean_ker = np.ones((5,5))/25\n",
    "mean_image = convolve_2d(noisy_image, mean_ker)\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.title('5x5 Mean Filtered Image')\n",
    "plt.imshow(mean_image, 'gray')\n",
    "\n",
    "# Code for 6.c\n",
    "med_image = signal.medfilt(noisy_image, kernel_size = (5,5))\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.title('5x5 Median Filtered Image')\n",
    "plt.imshow(med_image, 'gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments here\n",
    "\n",
    "Part 6(a):\n",
    "The median filter is a nonlinear system that can be seen in the example below:  \n",
    "x1[n] = [1,2,3] y1[n] = [2]  \n",
    "x2[n] = [1,-10,1] y2[n] = [1]  \n",
    "x1[n] + x2[n] = [2,-8,4] which is not equal to y1[n] + y2[n] = [3], the correct ansxwer should be 2  \n",
    "This proves that this system is non-linear because a linear system should have 2 properties. 1. When you add inputs, the output should be the 2 original outputs added together, and 2. When you multiply input by a constant, the output should be multiplied by the same constant as well. The above example doesn't follow rule 1. \n",
    "\n",
    "Part 6(d):  \n",
    "The median filter seems to work much better because there are way fewer distortion dots in the picture. I think this is the case because the median filter actually chooses one of the pixels in the original pictures, while the mean filter can choose values that are not in the original pictures. Furthermore, median filters are less affected by outliers, since salt and pepper distortion is full of extreme values, the mean filter can really mess up as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Instructions\n",
    "\n",
    "Make sure to place all image and data files along with your .ipynb lab report (this file) in one folder, zip the folder, and submit it to Canvas under the Lab 2 assignment. Please name the zip file ``<netid>_Lab2``."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
